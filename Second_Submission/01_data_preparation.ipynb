{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 01. 데이터 준비 및 기본 모델 훈련\n",
        "\n",
        "성능 목표: ROC-AUC 0.757 → 0.93+\n",
        "\n",
        "## 주요 개선사항\n",
        "- A100 GPU 최적화\n",
        "- 클래스 불균형 해결\n",
        "- 안정적 훈련 설정\n",
        "- 모델 저장 및 재사용 구조\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 라이브러리 설치\n",
        "%pip install transformers[torch] datasets accelerate -q\n",
        "%pip install scikit-learn xgboost lightgbm -q\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "\n",
        "# wandb 완전 제거\n",
        "%pip uninstall wandb -y -q\n",
        "\n",
        "print(\"라이브러리 설치 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 라이브러리 임포트\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, get_linear_schedule_with_warmup,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# wandb 완전 비활성화\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "\n",
        "# 시드 고정\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)\n",
        "print(\"환경 설정 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A100 GPU 확인 및 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# A100 최적화 설정\n",
        "MODEL_NAME = \"klue/roberta-large\"\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 32\n",
        "GRADIENT_ACCUMULATION = 2\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_STEPS = 1000\n",
        "MAX_GRAD_NORM = 1.0\n",
        "WEIGHT_DECAY = 0.01\n",
        "LABEL_SMOOTHING = 0.1\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "\n",
        "print(f\"모델: {MODEL_NAME}\")\n",
        "print(f\"배치 크기: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} (effective)\")\n",
        "print(f\"학습률: {LEARNING_RATE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 업로드\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 해제 및 로딩\n",
        "import zipfile\n",
        "\n",
        "# zip 파일 해제\n",
        "with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "train = pd.read_csv('train.csv', encoding='utf-8-sig')\n",
        "test = pd.read_csv('test.csv', encoding='utf-8-sig')\n",
        "\n",
        "print(f\"Training data: {train.shape}\")\n",
        "print(f\"Test data: {test.shape}\")\n",
        "print(f\"Generated distribution: {train['generated'].value_counts()}\")\n",
        "print(f\"Generated ratio: {train['generated'].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 문단 단위 분할 및 처리\n",
        "def split_text_to_paragraphs(text, min_length=50):\n",
        "    \"\"\"전체 글을 문단으로 분할\"\"\"\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    paragraphs = [p for p in paragraphs if len(p) >= min_length]\n",
        "\n",
        "    if not paragraphs:\n",
        "        sentences = text.split('. ')\n",
        "        paragraphs = []\n",
        "        current_para = \"\"\n",
        "        for sent in sentences:\n",
        "            current_para += sent + \". \"\n",
        "            if len(current_para) >= 200:\n",
        "                paragraphs.append(current_para.strip())\n",
        "                current_para = \"\"\n",
        "        if current_para.strip():\n",
        "            paragraphs.append(current_para.strip())\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "# 문단 단위 변환\n",
        "train_paragraphs = []\n",
        "for idx, row in train.iterrows():\n",
        "    paragraphs = split_text_to_paragraphs(row['full_text'])\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "        train_paragraphs.append({\n",
        "            'title': row['title'],\n",
        "            'paragraph_index': i,\n",
        "            'paragraph_text': paragraph,\n",
        "            'generated': row['generated']\n",
        "        })\n",
        "\n",
        "train_para_df = pd.DataFrame(train_paragraphs)\n",
        "print(f\"원본 훈련 데이터: {len(train)}개 글\")\n",
        "print(f\"변환된 훈련 데이터: {len(train_para_df)}개 문단\")\n",
        "print(f\"평균 문단 수: {len(train_para_df) / len(train):.1f}개/글\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 클래스 가중치 계산 및 데이터셋 클래스\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_para_df['generated']),\n",
        "    y=train_para_df['generated']\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "print(f\"클래스 가중치: {class_weight_dict}\")\n",
        "\n",
        "class AIDetectionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# title 기준 데이터 분할 (데이터 누수 방지)\n",
        "title_labels = train.set_index('title')['generated'].to_dict()\n",
        "unique_titles = list(title_labels.keys())\n",
        "title_y = [title_labels[title] for title in unique_titles]\n",
        "\n",
        "train_titles, val_titles = train_test_split(\n",
        "    unique_titles, test_size=0.2, random_state=42,\n",
        "    stratify=title_y\n",
        ")\n",
        "\n",
        "train_mask = train_para_df['title'].isin(train_titles)\n",
        "val_mask = train_para_df['title'].isin(val_titles)\n",
        "\n",
        "X_train = train_para_df[train_mask]['paragraph_text']\n",
        "y_train = train_para_df[train_mask]['generated']\n",
        "X_val = train_para_df[val_mask]['paragraph_text']\n",
        "y_val = train_para_df[val_mask]['generated']\n",
        "\n",
        "print(f\"훈련 제목: {len(train_titles)}개\")\n",
        "print(f\"검증 제목: {len(val_titles)}개\")\n",
        "print(f\"훈련 문단: {len(X_train)}개\")\n",
        "print(f\"검증 문단: {len(X_val)}개\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 및 토크나이저 로딩\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = AIDetectionDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
        "val_dataset = AIDetectionDataset(X_val, y_val, tokenizer, MAX_LENGTH)\n",
        "\n",
        "print(f\"데이터셋 생성 완료\")\n",
        "print(f\"훈련 샘플: {len(train_dataset)}\")\n",
        "print(f\"검증 샘플: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 가중치 적용 트레이너 및 평가 메트릭\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        class_weights_tensor = torch.tensor([\n",
        "            class_weight_dict[0],\n",
        "            class_weight_dict[1]\n",
        "        ], dtype=torch.float).to(logits.device)\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(\n",
        "            weight=class_weights_tensor,\n",
        "            label_smoothing=LABEL_SMOOTHING\n",
        "        )\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "    probs = torch.softmax(torch.tensor(eval_pred.predictions), dim=-1)[:, 1].numpy()\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "    except:\n",
        "        auc = 0.5\n",
        "\n",
        "    accuracy = (predictions == labels).mean()\n",
        "    return {'accuracy': accuracy, 'auc': auc}\n",
        "\n",
        "print(\"트레이너 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 훈련 설정 및 트레이너 초기화\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_step1',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    logging_dir='./logs_step1',\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_auc\",\n",
        "    greater_is_better=True,\n",
        "    dataloader_pin_memory=True,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=4,\n",
        "    report_to=[],\n",
        "    disable_tqdm=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "print(\"훈련 준비 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 훈련 실행\n",
        "print(\"기본 모델 훈련 시작...\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    \n",
        "    # 최종 평가\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"최종 검증 성능: {eval_results}\")\n",
        "    \n",
        "    # 모델 저장\n",
        "    model.save_pretrained('./base_model')\n",
        "    tokenizer.save_pretrained('./base_model')\n",
        "    \n",
        "    # 메타데이터 저장\n",
        "    metadata = {\n",
        "        'model_name': MODEL_NAME,\n",
        "        'eval_results': eval_results,\n",
        "        'class_weights': class_weight_dict,\n",
        "        'train_titles': train_titles,\n",
        "        'val_titles': val_titles\n",
        "    }\n",
        "    \n",
        "    with open('step1_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"기본 모델 훈련 및 저장 완료\")\n",
        "    print(f\"검증 AUC: {eval_results.get('eval_auc', 0):.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"훈련 중 오류: {e}\")\n",
        "    # 에러 발생 시에도 현재 모델 저장\n",
        "    model.save_pretrained('./base_model_backup')\n",
        "    tokenizer.save_pretrained('./base_model_backup')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 저장 (다음 단계에서 사용)\n",
        "train_para_df.to_pickle('train_para_df.pkl')\n",
        "test.to_pickle('test_df.pkl')\n",
        "\n",
        "# 추가 정보 저장\n",
        "with open('data_info.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'train_titles': train_titles,\n",
        "        'val_titles': val_titles,\n",
        "        'class_weight_dict': class_weight_dict\n",
        "    }, f)\n",
        "\n",
        "print(\"1단계 완료 - 기본 모델 훈련 및 데이터 준비\")\n",
        "print(\"다음 단계: 02_hierarchical_modeling.ipynb 실행\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
