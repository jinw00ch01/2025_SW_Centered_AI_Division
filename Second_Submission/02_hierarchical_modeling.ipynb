{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 02. 계층적 문서-문단 모델링\n",
        "\n",
        "성능 목표: 0.85 → 0.90 (계층적 구조 활용)\n",
        "\n",
        "## 핵심 개선사항\n",
        "- 글 단위 컨텍스트 모델링  \n",
        "- 문단 간 관계 학습\n",
        "- 순서 정보 활용\n",
        "- 일관성 강제 메커니즘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 라이브러리 및 이전 단계 데이터 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 이전 단계 데이터 로딩\n",
        "train_para_df = pd.read_pickle('train_para_df.pkl')\n",
        "test_df = pd.read_pickle('test_df.pkl')\n",
        "\n",
        "with open('data_info.pkl', 'rb') as f:\n",
        "    data_info = pickle.load(f)\n",
        "    train_titles = data_info['train_titles']\n",
        "    val_titles = data_info['val_titles']\n",
        "    class_weight_dict = data_info['class_weight_dict']\n",
        "\n",
        "with open('step1_metadata.json', 'r') as f:\n",
        "    step1_metadata = json.load(f)\n",
        "    MODEL_NAME = step1_metadata['model_name']\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"데이터 로딩 완료: {len(train_para_df)}개 문단, {len(test_df)}개 테스트 문단\")\n",
        "print(f\"모델: {MODEL_NAME}, 디바이스: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 계층적 AI 탐지 모델 정의\n",
        "class HierarchicalAIDetector(nn.Module):\n",
        "    \"\"\"글 단위 컨텍스트를 고려한 계층적 AI 탐지 모델\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, hidden_size=1024, num_heads=16):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 기본 인코더 (문단 수준)\n",
        "        self.roberta = AutoModel.from_pretrained(model_name)\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # 위치 임베딩 (문단 순서)\n",
        "        self.position_embedding = nn.Embedding(50, hidden_size)\n",
        "        \n",
        "        # 문단 간 어텐션 레이어 \n",
        "        self.inter_paragraph_attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        \n",
        "        # 문서 레벨 트랜스포머\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.document_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        \n",
        "        # 일관성 체크 레이어\n",
        "        self.consistency_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        \n",
        "        # 최종 분류기\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(hidden_size // 2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, paragraph_inputs, document_context=None, paragraph_positions=None):\n",
        "        batch_size, seq_len = paragraph_inputs['input_ids'].shape\n",
        "        \n",
        "        # 문단 수준 인코딩\n",
        "        with torch.cuda.amp.autocast():\n",
        "            paragraph_outputs = self.roberta(**paragraph_inputs)\n",
        "            paragraph_embeddings = paragraph_outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
        "        \n",
        "        # 위치 정보 추가\n",
        "        if paragraph_positions is not None:\n",
        "            pos_embeddings = self.position_embedding(paragraph_positions)\n",
        "            paragraph_embeddings = paragraph_embeddings + pos_embeddings\n",
        "        \n",
        "        # 문서 컨텍스트가 있는 경우 문단 간 관계 모델링\n",
        "        if document_context is not None:\n",
        "            # 문단들을 시퀀스로 처리\n",
        "            paragraph_sequence = paragraph_embeddings.unsqueeze(0)  # (1, num_paragraphs, hidden)\n",
        "            \n",
        "            # 문단 간 어텐션\n",
        "            attended_paragraphs, _ = self.inter_paragraph_attention(\n",
        "                paragraph_sequence, paragraph_sequence, paragraph_sequence\n",
        "            )\n",
        "            \n",
        "            # 문서 레벨 인코딩\n",
        "            document_representation = self.document_encoder(attended_paragraphs)\n",
        "            \n",
        "            # 원본과 문서 컨텍스트 결합\n",
        "            combined = torch.cat([\n",
        "                paragraph_embeddings,\n",
        "                document_representation.squeeze(0)\n",
        "            ], dim=-1)\n",
        "            \n",
        "            final_embeddings = self.consistency_layer(combined)\n",
        "        else:\n",
        "            final_embeddings = paragraph_embeddings\n",
        "        \n",
        "        # 분류\n",
        "        logits = self.classifier(final_embeddings)\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "print(\"계층적 모델 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 계층적 데이터셋 클래스\n",
        "class HierarchicalDataset(Dataset):\n",
        "    \"\"\"문서 컨텍스트를 고려한 데이터셋\"\"\"\n",
        "    \n",
        "    def __init__(self, para_df, tokenizer, max_length=512, mode='train'):\n",
        "        self.para_df = para_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.mode = mode\n",
        "        \n",
        "        # title별로 그룹화\n",
        "        self.grouped = para_df.groupby('title')\n",
        "        self.samples = []\n",
        "        \n",
        "        for title, group in self.grouped:\n",
        "            group = group.sort_values('paragraph_index')\n",
        "            for idx, row in group.iterrows():\n",
        "                self.samples.append({\n",
        "                    'title': title,\n",
        "                    'paragraph_text': row['paragraph_text'],\n",
        "                    'paragraph_index': row['paragraph_index'],\n",
        "                    'generated': row['generated'] if mode == 'train' else None,\n",
        "                    'other_paragraphs': group[group.index != idx]['paragraph_text'].tolist()\n",
        "                })\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # 현재 문단 토크나이징\n",
        "        paragraph_encoding = self.tokenizer(\n",
        "            sample['paragraph_text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # 문서 컨텍스트 (다른 문단들)\n",
        "        if sample['other_paragraphs']:\n",
        "            # 다른 문단들을 합쳐서 컨텍스트로 사용 (일부만)\n",
        "            context_text = ' '.join(sample['other_paragraphs'][:3])  # 최대 3개 문단\n",
        "            context_encoding = self.tokenizer(\n",
        "                context_text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        else:\n",
        "            # 컨텍스트가 없으면 현재 문단으로 대체\n",
        "            context_encoding = paragraph_encoding\n",
        "        \n",
        "        result = {\n",
        "            'paragraph_input_ids': paragraph_encoding['input_ids'].flatten(),\n",
        "            'paragraph_attention_mask': paragraph_encoding['attention_mask'].flatten(),\n",
        "            'context_input_ids': context_encoding['input_ids'].flatten(),\n",
        "            'context_attention_mask': context_encoding['attention_mask'].flatten(),\n",
        "            'paragraph_position': torch.tensor(sample['paragraph_index'], dtype=torch.long)\n",
        "        }\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            result['labels'] = torch.tensor(sample['generated'], dtype=torch.float)\n",
        "        \n",
        "        return result\n",
        "\n",
        "print(\"계층적 데이터셋 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 계층적 모델 훈련 함수\n",
        "def train_hierarchical_model(model, train_loader, val_loader, num_epochs=3, lr=1e-5):\n",
        "    \"\"\"계층적 모델 훈련\"\"\"\n",
        "    \n",
        "    # 옵티마이저 및 스케줄러\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    \n",
        "    # 클래스 가중치 적용한 손실 함수\n",
        "    class_weights_tensor = torch.tensor([\n",
        "        class_weight_dict[0], class_weight_dict[1]\n",
        "    ], dtype=torch.float).to(device)\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss(\n",
        "        pos_weight=class_weights_tensor[1] / class_weights_tensor[0]\n",
        "    )\n",
        "    \n",
        "    model.to(device)\n",
        "    best_auc = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # 훈련\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        \n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # 입력 준비\n",
        "            paragraph_inputs = {\n",
        "                'input_ids': batch['paragraph_input_ids'].to(device),\n",
        "                'attention_mask': batch['paragraph_attention_mask'].to(device)\n",
        "            }\n",
        "            context_inputs = {\n",
        "                'input_ids': batch['context_input_ids'].to(device),\n",
        "                'attention_mask': batch['context_attention_mask'].to(device)\n",
        "            }\n",
        "            positions = batch['paragraph_position'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            # 순전파\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(\n",
        "                    paragraph_inputs,\n",
        "                    document_context=context_inputs,\n",
        "                    paragraph_positions=positions\n",
        "                )\n",
        "                loss = criterion(outputs, labels)\n",
        "            \n",
        "            # 역전파\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # 검증\n",
        "        model.eval()\n",
        "        val_predictions = []\n",
        "        val_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                paragraph_inputs = {\n",
        "                    'input_ids': batch['paragraph_input_ids'].to(device),\n",
        "                    'attention_mask': batch['paragraph_attention_mask'].to(device)\n",
        "                }\n",
        "                context_inputs = {\n",
        "                    'input_ids': batch['context_input_ids'].to(device),\n",
        "                    'attention_mask': batch['context_attention_mask'].to(device)\n",
        "                }\n",
        "                positions = batch['paragraph_position'].to(device)\n",
        "                labels = batch['labels']\n",
        "                \n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(\n",
        "                        paragraph_inputs,\n",
        "                        document_context=context_inputs,\n",
        "                        paragraph_positions=positions\n",
        "                    )\n",
        "                \n",
        "                predictions = torch.sigmoid(outputs).cpu().numpy()\n",
        "                val_predictions.extend(predictions)\n",
        "                val_labels.extend(labels.numpy())\n",
        "        \n",
        "        # AUC 계산\n",
        "        val_auc = roc_auc_score(val_labels, val_predictions)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "        print(f\"  Val AUC: {val_auc:.4f}\")\n",
        "        \n",
        "        # 최고 성능 모델 저장\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            torch.save(model.state_dict(), 'hierarchical_model_best.pth')\n",
        "    \n",
        "    return best_auc\n",
        "\n",
        "print(\"훈련 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 준비 및 모델 생성\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 훈련/검증 데이터 분할\n",
        "train_mask = train_para_df['title'].isin(train_titles)\n",
        "val_mask = train_para_df['title'].isin(val_titles)\n",
        "\n",
        "train_data = train_para_df[train_mask]\n",
        "val_data = train_para_df[val_mask]\n",
        "\n",
        "print(f\"훈련 데이터: {len(train_data)}개 문단\")\n",
        "print(f\"검증 데이터: {len(val_data)}개 문단\")\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = HierarchicalDataset(train_data, tokenizer, mode='train')\n",
        "val_dataset = HierarchicalDataset(val_data, tokenizer, mode='train')\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 계층적 모델 생성\n",
        "hierarchical_model = HierarchicalAIDetector(MODEL_NAME)\n",
        "\n",
        "print(f\"계층적 모델 생성 완료\")\n",
        "print(f\"훈련 배치 수: {len(train_loader)}\")\n",
        "print(f\"검증 배치 수: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 계층적 모델 훈련 실행\n",
        "print(\"계층적 모델 훈련 시작...\")\n",
        "\n",
        "try:\n",
        "    best_auc = train_hierarchical_model(\n",
        "        hierarchical_model, \n",
        "        train_loader, \n",
        "        val_loader,\n",
        "        num_epochs=3,\n",
        "        lr=1e-5\n",
        "    )\n",
        "    \n",
        "    print(f\"계층적 모델 훈련 완료\")\n",
        "    print(f\"최고 검증 AUC: {best_auc:.4f}\")\n",
        "    \n",
        "    # 메타데이터 저장\n",
        "    hierarchical_metadata = {\n",
        "        'model_type': 'hierarchical',\n",
        "        'best_auc': best_auc,\n",
        "        'model_name': MODEL_NAME,\n",
        "        'improvements': [\n",
        "            'document_context_modeling',\n",
        "            'inter_paragraph_attention', \n",
        "            'position_encoding',\n",
        "            'consistency_layer'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with open('step2_metadata.json', 'w') as f:\n",
        "        json.dump(hierarchical_metadata, f, indent=2)\n",
        "    \n",
        "    print(\"2단계 완료 - 계층적 모델링\")\n",
        "    print(\"다음 단계: 03_korean_features.ipynb 실행\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"훈련 중 오류: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
