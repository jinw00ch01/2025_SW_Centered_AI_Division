{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 03. 한국어 AI 특화 특성 공학\n",
        "\n",
        "성능 목표: 0.90 → 0.92 (언어학적 특성 활용)\n",
        "\n",
        "## 핵심 개선사항  \n",
        "- 한국어 어미 패턴 분석\n",
        "- 접속사 및 전이 표현 특성\n",
        "- 어휘 다양성 지수\n",
        "- 문체 일관성 측정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 라이브러리 및 데이터 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 이전 단계 데이터 로딩\n",
        "train_para_df = pd.read_pickle('train_para_df.pkl')\n",
        "test_df = pd.read_pickle('test_df.pkl')\n",
        "\n",
        "with open('data_info.pkl', 'rb') as f:\n",
        "    data_info = pickle.load(f)\n",
        "    train_titles = data_info['train_titles']\n",
        "    val_titles = data_info['val_titles']\n",
        "\n",
        "print(f\"데이터 로딩 완료\")\n",
        "print(f\"훈련 데이터: {len(train_para_df)}개 문단\")\n",
        "print(f\"테스트 데이터: {len(test_df)}개 문단\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 한국어 AI 특화 특성 추출 함수들\n",
        "def extract_korean_ai_features(text):\n",
        "    \"\"\"한국어 AI 생성 텍스트의 특징을 추출\"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return np.zeros(40)\n",
        "    \n",
        "    features = []\n",
        "    words = text.split()\n",
        "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "    \n",
        "    # 1. 기본 통계 (5개)\n",
        "    features.extend([\n",
        "        len(words),\n",
        "        len(sentences), \n",
        "        len(words) / len(sentences) if sentences else 0,\n",
        "        len(set(words)) / len(words) if words else 0,  # 어휘 다양성\n",
        "        np.std([len(w) for w in words]) if words else 0  # 단어 길이 변동성\n",
        "    ])\n",
        "    \n",
        "    # 2. 한국어 어미 패턴 분석 (10개)\n",
        "    formal_endings = ['습니다', '입니다', '했습니다', '있습니다', '됩니다']\n",
        "    informal_endings = ['다', '야', '지', '네', '요']\n",
        "    question_endings = ['까', '나', '니']\n",
        "    \n",
        "    formal_count = sum(text.count(ending) for ending in formal_endings)\n",
        "    informal_count = sum(text.count(ending) for ending in informal_endings)\n",
        "    question_count = sum(text.count(ending) for ending in question_endings)\n",
        "    \n",
        "    features.extend([\n",
        "        formal_count / len(sentences) if sentences else 0,\n",
        "        informal_count / len(sentences) if sentences else 0, \n",
        "        question_count / len(sentences) if sentences else 0,\n",
        "        formal_count / (formal_count + informal_count + 1),  # 격식체 비율\n",
        "        len(set([w[-2:] for w in words if len(w) >= 2])) / len(words) if words else 0,  # 어미 다양성\n",
        "        text.count('다') / len(words) if words else 0,\n",
        "        text.count('습니다') / len(sentences) if sentences else 0,\n",
        "        text.count('입니다') / len(sentences) if sentences else 0,\n",
        "        text.count('했습니다') / len(sentences) if sentences else 0,\n",
        "        text.count('있습니다') / len(sentences) if sentences else 0\n",
        "    ])\n",
        "    \n",
        "    # 3. 접속사 및 전이 표현 (8개)\n",
        "    conjunctions = ['그리고', '또한', '따라서', '그러나', '하지만', '그러므로', '그런데', '즉']\n",
        "    transition_words = ['첫째', '둘째', '셋째', '마지막으로', '결국', '결론적으로']\n",
        "    \n",
        "    conjunction_count = sum(text.count(conj) for conj in conjunctions)\n",
        "    transition_count = sum(text.count(trans) for trans in transition_words)\n",
        "    \n",
        "    features.extend([\n",
        "        conjunction_count / len(sentences) if sentences else 0,\n",
        "        transition_count / len(sentences) if sentences else 0,\n",
        "        text.count('그리고') / len(sentences) if sentences else 0,\n",
        "        text.count('또한') / len(sentences) if sentences else 0,\n",
        "        text.count('따라서') / len(sentences) if sentences else 0,\n",
        "        text.count('하지만') / len(sentences) if sentences else 0,\n",
        "        len(set(conjunctions) & set(words)) / len(conjunctions),  # 접속사 다양성\n",
        "        (conjunction_count + transition_count) / len(words) if words else 0\n",
        "    ])\n",
        "    \n",
        "    # 4. 조사 패턴 (7개)\n",
        "    particles = ['은', '는', '이', '가', '을', '를', '에', '의', '로', '와', '과']\n",
        "    particle_counts = [text.count(p) for p in particles]\n",
        "    \n",
        "    features.extend([\n",
        "        sum(particle_counts) / len(words) if words else 0,\n",
        "        np.std(particle_counts) if particle_counts else 0,\n",
        "        text.count('의') / len(words) if words else 0,\n",
        "        text.count('에') / len(words) if words else 0,\n",
        "        text.count('은') / len(words) if words else 0,\n",
        "        text.count('는') / len(words) if words else 0,\n",
        "        (text.count('은') + text.count('는')) / (text.count('이') + text.count('가') + 1)  # 주제/주어 조사 비율\n",
        "    ])\n",
        "    \n",
        "    # 5. 문장 구조 및 길이 패턴 (5개)\n",
        "    sentence_lengths = [len(s.split()) for s in sentences] if sentences else [0]\n",
        "    \n",
        "    features.extend([\n",
        "        np.mean(sentence_lengths),\n",
        "        np.std(sentence_lengths),\n",
        "        len([s for s in sentences if len(s.split()) > 20]) / len(sentences) if sentences else 0,  # 긴 문장 비율\n",
        "        len([s for s in sentences if len(s.split()) < 5]) / len(sentences) if sentences else 0,   # 짧은 문장 비율\n",
        "        max(sentence_lengths) / np.mean(sentence_lengths) if sentence_lengths and np.mean(sentence_lengths) > 0 else 0\n",
        "    ])\n",
        "    \n",
        "    # 6. 반복 및 일관성 패턴 (5개)\n",
        "    word_freq = Counter(words)\n",
        "    most_common = word_freq.most_common(5)\n",
        "    \n",
        "    features.extend([\n",
        "        len([w for w, c in word_freq.items() if c > 1]) / len(words) if words else 0,  # 반복 단어 비율\n",
        "        most_common[0][1] / len(words) if most_common else 0,  # 최빈 단어 비율\n",
        "        len([w for w in words if len(w) > 5]) / len(words) if words else 0,  # 긴 단어 비율\n",
        "        text.count('것') / len(words) if words else 0,  # '것' 의존도\n",
        "        len(re.findall(r'[0-9]+', text)) / len(words) if words else 0  # 숫자 사용 비율\n",
        "    ])\n",
        "    \n",
        "    return np.array(features, dtype=np.float32)\n",
        "\n",
        "print(\"한국어 특화 특성 추출 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 훈련 데이터에서 한국어 특성 추출\n",
        "print(\"훈련 데이터 한국어 특성 추출 시작...\")\n",
        "\n",
        "train_mask = train_para_df['title'].isin(train_titles)\n",
        "val_mask = train_para_df['title'].isin(val_titles)\n",
        "\n",
        "train_data = train_para_df[train_mask]\n",
        "val_data = train_para_df[val_mask]\n",
        "\n",
        "# 특성 추출\n",
        "train_korean_features = []\n",
        "for text in train_data['paragraph_text']:\n",
        "    features = extract_korean_ai_features(text)\n",
        "    train_korean_features.append(features)\n",
        "\n",
        "val_korean_features = []\n",
        "for text in val_data['paragraph_text']:\n",
        "    features = extract_korean_ai_features(text)\n",
        "    val_korean_features.append(features)\n",
        "\n",
        "X_train_korean = np.array(train_korean_features)\n",
        "X_val_korean = np.array(val_korean_features)\n",
        "y_train = train_data['generated'].values\n",
        "y_val = val_data['generated'].values\n",
        "\n",
        "print(f\"훈련 특성 shape: {X_train_korean.shape}\")\n",
        "print(f\"검증 특성 shape: {X_val_korean.shape}\")\n",
        "print(f\"특성 개수: {X_train_korean.shape[1]}개\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 특성 정규화 및 모델 훈련\n",
        "print(\"한국어 특성 기반 모델 훈련...\")\n",
        "\n",
        "# 특성 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_korean)\n",
        "X_val_scaled = scaler.transform(X_val_korean)\n",
        "\n",
        "# Random Forest 모델 훈련 (한국어 특성에 적합)\n",
        "korean_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "korean_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 검증 성능 평가\n",
        "val_predictions = korean_model.predict_proba(X_val_scaled)[:, 1]\n",
        "korean_auc = roc_auc_score(y_val, val_predictions)\n",
        "\n",
        "print(f\"한국어 특성 모델 검증 AUC: {korean_auc:.4f}\")\n",
        "\n",
        "# 특성 중요도 분석\n",
        "feature_names = [\n",
        "    'word_count', 'sentence_count', 'words_per_sentence', 'vocabulary_diversity', 'word_length_std',\n",
        "    'formal_endings_rate', 'informal_endings_rate', 'question_endings_rate', 'formal_ratio', 'ending_diversity',\n",
        "    'da_frequency', 'seumnida_rate', 'imnida_rate', 'haetseumnida_rate', 'itseumnida_rate',\n",
        "    'conjunction_rate', 'transition_rate', 'geurigo_rate', 'tohan_rate', 'ttaraseo_rate', 'hajiman_rate', 'conjunction_diversity', 'logical_words_rate',\n",
        "    'particle_rate', 'particle_std', 'ui_frequency', 'e_frequency', 'eun_frequency', 'neun_frequency', 'topic_subject_ratio',\n",
        "    'avg_sentence_length', 'sentence_length_std', 'long_sentence_ratio', 'short_sentence_ratio', 'length_variation',\n",
        "    'repeated_word_ratio', 'most_common_ratio', 'long_word_ratio', 'geot_dependency', 'number_ratio'\n",
        "]\n",
        "\n",
        "importances = korean_model.feature_importances_\n",
        "top_features = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"상위 10개 중요 특성:\")\n",
        "for name, importance in top_features:\n",
        "    print(f\"  {name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터 특성 추출 및 예측\n",
        "print(\"테스트 데이터 한국어 특성 추출...\")\n",
        "\n",
        "test_korean_features = []\n",
        "for text in test_df['paragraph_text']:\n",
        "    features = extract_korean_ai_features(text)\n",
        "    test_korean_features.append(features)\n",
        "\n",
        "X_test_korean = np.array(test_korean_features)\n",
        "X_test_scaled = scaler.transform(X_test_korean)\n",
        "\n",
        "# 테스트 예측\n",
        "test_predictions = korean_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(f\"테스트 특성 shape: {X_test_korean.shape}\")\n",
        "print(f\"예측 통계:\")\n",
        "print(f\"  평균: {test_predictions.mean():.4f}\")\n",
        "print(f\"  표준편차: {test_predictions.std():.4f}\")\n",
        "print(f\"  최소값: {test_predictions.min():.4f}\")\n",
        "print(f\"  최대값: {test_predictions.max():.4f}\")\n",
        "\n",
        "# 결과 저장\n",
        "korean_results = {\n",
        "    'train_features': X_train_korean,\n",
        "    'val_features': X_val_korean, \n",
        "    'test_features': X_test_korean,\n",
        "    'scaler': scaler,\n",
        "    'model': korean_model,\n",
        "    'val_auc': korean_auc,\n",
        "    'test_predictions': test_predictions,\n",
        "    'feature_names': feature_names,\n",
        "    'feature_importances': importances\n",
        "}\n",
        "\n",
        "with open('korean_features_results.pkl', 'wb') as f:\n",
        "    pickle.dump(korean_results, f)\n",
        "\n",
        "# 메타데이터 저장\n",
        "korean_metadata = {\n",
        "    'model_type': 'korean_features',\n",
        "    'val_auc': korean_auc,\n",
        "    'feature_count': X_train_korean.shape[1],\n",
        "    'top_features': top_features[:5],\n",
        "    'improvements': [\n",
        "        'korean_ending_patterns',\n",
        "        'conjunction_analysis', \n",
        "        'particle_patterns',\n",
        "        'vocabulary_diversity',\n",
        "        'sentence_structure'\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('step3_metadata.json', 'w') as f:\n",
        "    json.dump(korean_metadata, f, indent=2)\n",
        "\n",
        "print(\"3단계 완료 - 한국어 특화 특성 공학\")\n",
        "print(\"다음 단계: 04_advanced_ensemble.ipynb 실행\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
