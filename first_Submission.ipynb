{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de9b3f5",
   "metadata": {},
   "source": [
    "# ğŸš€ í•œêµ­ì–´ AI ìƒì„± í…ìŠ¤íŠ¸ íƒì§€ ëª¨ë¸ (A100 GPU ìµœì í™” + ëª¨ë¸ ì•ˆì •í™”)\n",
    "\n",
    "## ğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- âœ… **A100 GPU ìµœì í™”**: Large ëª¨ë¸, í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ, ê¸´ ì‹œí€€ìŠ¤\n",
    "- âœ… **ëª¨ë¸ ë¶•ê´´ ë°©ì§€**: Learning rate ì¡°ì •, Gradient clipping, Early stopping\n",
    "- âœ… **í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°**: Class weighting, Label smoothing\n",
    "- âœ… **ì•ˆì •ì  ìµœì í™”**: Improved optimizer, Warm-up scheduling\n",
    "- ğŸš« **ì™„ì „ ë¡œì»¬ í™˜ê²½**: wandb ì™„ì „ ì°¨ë‹¨, ì™¸ë¶€ API ì—†ìŒ\n",
    "\n",
    "## âš ï¸ ì´ì „ ë²„ì „ ë¬¸ì œì  ë¶„ì„\n",
    "- **ëª¨ë¸ ë¶•ê´´**: Step 3500ë¶€í„° AUC 0.5, Accuracy ê³ ì • (0.917726)\n",
    "- **Learning rate ê³¼ë„**: 5e-5 â†’ ëª¨ë¸ ë¶ˆì•ˆì •ì„± ì•¼ê¸°\n",
    "- **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: Majority class biasë¡œ ì¸í•œ ì˜ˆì¸¡ í¸í–¥\n",
    "- **T4 ìµœì í™” í•œê³„**: ì‘ì€ ëª¨ë¸ê³¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "\n",
    "## ğŸ”§ í•µì‹¬ í•´ê²°ì±…\n",
    "1. **Learning Rate**: 5e-5 â†’ 2e-5 (ì•ˆì •í™”)\n",
    "2. **Gradient Clipping**: max_norm=1.0 (í­ë°œ ë°©ì§€)\n",
    "3. **Class Weighting**: ë¶ˆê· í˜• ìë™ ë³´ì •\n",
    "4. **Early Stopping**: patience=3 (ê³¼ì í•© ë°©ì§€)\n",
    "5. **A100 ìµœì í™”**: Large model + ë°°ì¹˜ 64 + ì‹œí€€ìŠ¤ 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff00051",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 GPU í™˜ê²½ì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install transformers[torch] datasets accelerate\n",
    "%pip install scikit-learn xgboost\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# wandb ì™„ì „ ì œê±°\n",
    "%pip uninstall wandb -y\n",
    "\n",
    "print(\"wandb ì™„ì „ ì œê±° ì™„ë£Œ!\")\n",
    "print(\"A100 GPU ìµœì í™” í™˜ê²½ êµ¬ì¶• ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d801b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 GPU í™•ì¸\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "    # A100 í™•ì¸\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if \"A100\" in gpu_name:\n",
    "        print(\"A100 GPU ê°ì§€! ê³ ì„±ëŠ¥ ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(f\"A100ì´ ì•„ë‹Œ GPU ê°ì§€: {gpu_name}\")\n",
    "        print(\"ê·¸ë˜ë„ í–¥ìƒëœ ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c553d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° wandb ì™„ì „ ì°¨ë‹¨\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, get_linear_schedule_with_warmup,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637a6fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# wandb ì™„ì „ ë¹„í™œì„±í™” (ë‹¤ì¸µ ë°©ì–´)\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'disabled'\n",
    "os.environ['WANDB_PROJECT'] = 'disabled'\n",
    "\n",
    "# transformers integrations ì™„ì „ ë¹„í™œì„±í™”\n",
    "try:\n",
    "    from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "    for integration_name in list(INTEGRATION_TO_CALLBACK.keys()):\n",
    "        if integration_name in ['wandb', 'tensorboard', 'comet', 'mlflow']:\n",
    "            del INTEGRATION_TO_CALLBACK[integration_name]\n",
    "            print(f\"{integration_name} í†µí•© ì™„ì „ ë¹„í™œì„±í™”\")\n",
    "except Exception as e:\n",
    "    print(f\"Integration ë¹„í™œì„±í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39743c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ì‹œë“œ ê³ ì •\n",
    "import random\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"A100 ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(\"wandb ì™„ì „ ë¹„í™œì„±í™” - ëª¨ë“  ë¡œê¹…ì„ ë¡œì»¬ì—ì„œë§Œ ì²˜ë¦¬!\")\n",
    "print(\"ëª¨ë“  ì™¸ë¶€ API ì—°ê²°ì´ ì™„ì „íˆ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926321f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 GPU ìµœì í™” ì„¤ì • (ëª¨ë¸ ë¶•ê´´ ë°©ì§€ í¬í•¨)\n",
    "\n",
    "# A100 ìµœì í™” ëª¨ë¸ ì„¤ì •\n",
    "MODEL_NAME = \"klue/roberta-large\"    # T4: base -> A100: large (3ë°° ì„±ëŠ¥ í–¥ìƒ)\n",
    "MAX_LENGTH = 512                      # T4: 256 -> A100: 512 (2ë°° ì»¨í…ìŠ¤íŠ¸)\n",
    "BATCH_SIZE = 32                       # T4: 16 -> A100: 32 (A100ì˜ 40GB ë©”ëª¨ë¦¬ í™œìš©)\n",
    "GRADIENT_ACCUMULATION = 2             # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° = 64\n",
    "\n",
    "# ëª¨ë¸ ë¶•ê´´ ë°©ì§€ ì„¤ì • (ì´ì „ ë¬¸ì œ í•´ê²°)\n",
    "LEARNING_RATE = 2e-5                 # í•µì‹¬: 5e-5 -> 2e-5 (ì•ˆì •í™”)\n",
    "WARMUP_STEPS = 1000                  # ì¶©ë¶„í•œ warm-upìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\n",
    "MAX_GRAD_NORM = 1.0                  # Gradient clipping (í­ë°œ ë°©ì§€)\n",
    "WEIGHT_DECAY = 0.01                  # ì •ê·œí™” ê°•í™”\n",
    "LABEL_SMOOTHING = 0.1                # ê³¼ì‹  ë°©ì§€\n",
    "EARLY_STOPPING_PATIENCE = 3          # ì¡°ê¸° ì¢…ë£Œ\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"A100 ìµœì í™” ì„¤ì •:\")\n",
    "print(f\"   ëª¨ë¸: {MODEL_NAME}\")\n",
    "print(f\"   ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_LENGTH}\")\n",
    "print(f\"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} (effective)\")\n",
    "print(f\"   í•™ìŠµë¥ : {LEARNING_RATE} (ì•ˆì •í™”ë¨)\")\n",
    "print(f\"   Gradient clipping: {MAX_GRAD_NORM}\")\n",
    "print(f\"   Early stopping: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"   ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡\n",
    "if torch.cuda.is_available():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    estimated_usage = BATCH_SIZE * MAX_LENGTH * 1024 * 4 / 1024**3 * 1.5  # ëŒ€ëµì  ê³„ì‚°\n",
    "    print(f\"\\në©”ëª¨ë¦¬ ë¶„ì„:\")\n",
    "    print(f\"   ì‚¬ìš© ê°€ëŠ¥: {total_memory:.1f} GB\")\n",
    "    print(f\"   ì˜ˆìƒ ì‚¬ìš©ëŸ‰: {estimated_usage:.1f} GB\")\n",
    "    print(f\"   ì—¬ìœ ë„: {(total_memory - estimated_usage) / total_memory * 100:.1f}%\")\n",
    "\n",
    "    if estimated_usage > total_memory * 0.9:\n",
    "        print(\"ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜! ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”.\")\n",
    "    else:\n",
    "        print(\"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì „ ë²”ìœ„!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab9ec5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470a7ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# zip íŒŒì¼ í•´ì œ\n",
    "!unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6e346",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë”©\n",
    "train = pd.read_csv('train.csv', encoding='utf-8-sig')\n",
    "test = pd.read_csv('test.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(\"=== Training Data Analysis ===\")\n",
    "print(f\"Shape: {train.shape}\")\n",
    "print(f\"Generated distribution:\\n{train['generated'].value_counts()}\")\n",
    "print(f\"Generated ratio: {train['generated'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec38e3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¶•ê´´ ì›ì¸ ë¶„ì„\n",
    "print(\"\\n=== ëª¨ë¸ ë¶•ê´´ ì›ì¸ ë¶„ì„ ===\")\n",
    "class_counts = train['generated'].value_counts()\n",
    "majority_ratio = class_counts.max() / len(train)\n",
    "minority_ratio = class_counts.min() / len(train)\n",
    "\n",
    "print(f\"Majority class (0) ë¹„ìœ¨: {majority_ratio:.6f}\")\n",
    "print(f\"Minority class (1) ë¹„ìœ¨: {minority_ratio:.6f}\")\n",
    "print(f\"ë¶ˆê· í˜• ë¹„ìœ¨: {majority_ratio / minority_ratio:.2f}:1\")\n",
    "\n",
    "# ì´ì „ ê³ ì •ëœ accuracyì™€ ë¹„êµ\n",
    "print(f\"\\n ì´ì „ ëª¨ë¸ ë¶•ê´´ ë¶„ì„:\")\n",
    "print(f\"   ê³ ì •ëœ accuracy: 0.917726\")\n",
    "print(f\"   Majority ratio:   {majority_ratio:.6f}\")\n",
    "print(f\"   ì°¨ì´: {abs(majority_ratio - 0.917726):.6f}\")\n",
    "\n",
    "if abs(majority_ratio - 0.917726) < 0.01:\n",
    "    print(\"   í™•ì¸ë¨: ëª¨ë¸ì´ ëª¨ë“  ì˜ˆì¸¡ì„ majority classë¡œ í•¨!\")\n",
    "    print(\"   í•´ê²°ì±…: Class weighting + Label smoothing ì ìš©\")\n",
    "else:\n",
    "    print(\"   ë‹¤ë¥¸ ì›ì¸ì¼ ìˆ˜ë„ ìˆìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f629f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (í•µì‹¬ í•´ê²°ì±…)\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train['generated']),\n",
    "    y=train['generated']\n",
    ")\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(f\"\\nê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:\")\n",
    "print(f\"   Class 0 (majority): {class_weight_dict[0]:.3f}\")\n",
    "print(f\"   Class 1 (minority): {class_weight_dict[1]:.3f}\")\n",
    "print(f\"   ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {class_weight_dict[1]/class_weight_dict[0]:.2f}:1\")\n",
    "print(\"   â†’ ì´ ê°€ì¤‘ì¹˜ë¡œ ë¶ˆê· í˜•ì„ ë³´ì •í•˜ì—¬ ëª¨ë¸ ë¶•ê´´ ë°©ì§€!\")\n",
    "\n",
    "print(f\"\\n=== Test Data Analysis ===\")\n",
    "print(f\"Shape: {test.shape}\")\n",
    "print(f\"Unique titles: {test['title'].nunique()}\")\n",
    "print(f\"Total paragraphs: {len(test)}\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ í™•ì¸\n",
    "try:\n",
    "    sample_submission = pd.read_csv('./sample_submission.csv')\n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"Expected format: {sample_submission.columns.tolist()}\")\n",
    "except:\n",
    "    print(\"sample_submission.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b776eed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬: ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• \n",
    "def split_text_to_paragraphs(text, min_length=50):\n",
    "    \"\"\"ì „ì²´ ê¸€ì„ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• \"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    paragraphs = [p for p in paragraphs if len(p) >= min_length]\n",
    "\n",
    "    if not paragraphs:\n",
    "        sentences = text.split('. ')\n",
    "        paragraphs = []\n",
    "        current_para = \"\"\n",
    "        for sent in sentences:\n",
    "            current_para += sent + \". \"\n",
    "            if len(current_para) >= 200:\n",
    "                paragraphs.append(current_para.strip())\n",
    "                current_para = \"\"\n",
    "        if current_para.strip():\n",
    "            paragraphs.append(current_para.strip())\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962b9b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë¬¸ë‹¨ ë‹¨ìœ„ ë³€í™˜\n",
    "train_paragraphs = []\n",
    "for idx, row in train.iterrows():\n",
    "    paragraphs = split_text_to_paragraphs(row['full_text'])\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        train_paragraphs.append({\n",
    "            'title': row['title'],\n",
    "            'paragraph_index': i,\n",
    "            'paragraph_text': paragraph,\n",
    "            'generated': row['generated']\n",
    "        })\n",
    "\n",
    "train_para_df = pd.DataFrame(train_paragraphs)\n",
    "print(f\"ì›ë³¸ í›ˆë ¨ ë°ì´í„°: {len(train)}ê°œ ê¸€\")\n",
    "print(f\"ë³€í™˜ëœ í›ˆë ¨ ë°ì´í„°: {len(train_para_df)}ê°œ ë¬¸ë‹¨\")\n",
    "print(f\"í‰ê·  ë¬¸ë‹¨ ìˆ˜: {len(train_para_df) / len(train):.1f}ê°œ/ê¸€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391389b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë³€í™˜ í›„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "print(f\"ë³€í™˜ í›„ ë¼ë²¨ ë¶„í¬:\")\n",
    "print(train_para_df['generated'].value_counts())\n",
    "\n",
    "para_majority_ratio = train_para_df['generated'].value_counts().max() / len(train_para_df)\n",
    "print(f\"ë¬¸ë‹¨ ë‹¨ìœ„ majority ë¹„ìœ¨: {para_majority_ratio:.6f}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¬ê³„ì‚° (ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ)\n",
    "para_class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_para_df['generated']),\n",
    "    y=train_para_df['generated']\n",
    ")\n",
    "para_class_weight_dict = {i: para_class_weights[i] for i in range(len(para_class_weights))}\n",
    "print(f\"ë¬¸ë‹¨ ë‹¨ìœ„ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {para_class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca5215",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© (A100 ìµœì í™”)\n",
    "print(f\"ëª¨ë¸ ë¡œë”©: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ê³ ë ¤í•œ ê°œì„ ëœ ë°ì´í„°ì…‹\n",
    "class ImprovedAIDetectionDataset(Dataset):\n",
    "    \"\"\"í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ ê°œì„ ëœ ë°ì´í„°ì…‹\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512, class_weights=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        # í† í¬ë‚˜ì´ì§• (A100ì—ì„œ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c5432",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í•  (ê³„ì¸µí™” ë¶„í• ë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_para_df['paragraph_text'],\n",
    "    train_para_df['generated'],\n",
    "    test_size=0.2,\n",
    "    stratify=train_para_df['generated'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"í›ˆë ¨ ì„¸íŠ¸: {len(X_train)} ê°œ ë¬¸ë‹¨\")\n",
    "print(f\"ê²€ì¦ ì„¸íŠ¸: {len(X_val)} ê°œ ë¬¸ë‹¨\")\n",
    "print(f\"í›ˆë ¨ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:\\n{y_train.value_counts()}\")\n",
    "print(f\"ê²€ì¦ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:\\n{y_val.value_counts()}\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„± (A100 ìµœì í™” ì„¤ì •)\n",
    "train_dataset = ImprovedAIDetectionDataset(\n",
    "    X_train, y_train, tokenizer, MAX_LENGTH, para_class_weight_dict\n",
    ")\n",
    "val_dataset = ImprovedAIDetectionDataset(\n",
    "    X_val, y_val, tokenizer, MAX_LENGTH, para_class_weight_dict\n",
    ")\n",
    "\n",
    "print(f\"A100 ìµœì í™” ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_LENGTH} (T4 ëŒ€ë¹„ 2ë°°)\")\n",
    "print(f\"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©: {para_class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620cda1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™” ë° ì•ˆì •í™” ì„¤ì • (ëª¨ë¸ ë¶•ê´´ ë°©ì§€)\n",
    "print(\"A100 ìµœì í™” ëª¨ë¸ ì´ˆê¸°í™”...\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”© (A100ì—ì„œ large ëª¨ë¸ ì‚¬ìš©)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    "class_weights_tensor = torch.tensor([\n",
    "    para_class_weight_dict[0],\n",
    "    para_class_weight_dict[1]\n",
    "], dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ í…ì„œ: {class_weights_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b1ca3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ CrossEntropyLoss\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=LABEL_SMOOTHING  # ê³¼ì‹  ë°©ì§€\n",
    "        )\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc748428",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ì•ˆì •í™”ëœ í›ˆë ¨ ì„¤ì • (ëª¨ë¸ ë¶•ê´´ ì™„ì „ ë°©ì§€)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./a100_results',\n",
    "    num_train_epochs=5,                    # A100ìœ¼ë¡œ ë” ë§ì€ ì—í¬í¬ ê°€ëŠ¥\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "\n",
    "    # ì•ˆì •í™” ì„¤ì • (í•µì‹¬)\n",
    "    learning_rate=LEARNING_RATE,          # 2e-5 (ì•ˆì •í™”ë¨)\n",
    "    weight_decay=WEIGHT_DECAY,            # ì •ê·œí™” ê°•í™”\n",
    "    warmup_steps=WARMUP_STEPS,            # ì¶©ë¶„í•œ warm-up\n",
    "    max_grad_norm=MAX_GRAD_NORM,          # Gradient clipping\n",
    "\n",
    "    # ë¡œê¹… ë° í‰ê°€ ì„¤ì •\n",
    "    logging_dir='./a100_logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,                       # ë” ìì£¼ í‰ê°€\n",
    "    save_steps=200,                       # eval_stepsì˜ ë°°ìˆ˜ë¡œ ë³€ê²½\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_auc\",     # AUC ê¸°ì¤€ ìµœì í™”\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # A100 ìµœì í™”\n",
    "    dataloader_pin_memory=True,           # A100ì—ì„œ ë¹ ë¥¸ ë°ì´í„° ë¡œë”©\n",
    "    fp16=True,                            # Mixed precision\n",
    "    dataloader_num_workers=4,             # ë³‘ë ¬ ì²˜ë¦¬\n",
    "\n",
    "    # ì™„ì „ ë¡œì»¬ ì„¤ì •\n",
    "    report_to=[],                         # ëª¨ë“  ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”\n",
    "    disable_tqdm=False,\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"ì•ˆì •í™”ëœ í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"   í•™ìŠµë¥ : {LEARNING_RATE} (ì•ˆì •í™”)\")\n",
    "print(f\"   Gradient clipping: {MAX_GRAD_NORM}\")\n",
    "print(f\"   Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"   Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ì ìš©ë¨\")\n",
    "print(f\"   íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a17fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ì™„ì „ ë¡œì»¬ ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ + í‰ê°€ ë©”íŠ¸ë¦­\n",
    "class LocalLogger:\n",
    "    \"\"\"ì™„ì „ ë¡œì»¬ ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ (ì™¸ë¶€ ì„œë²„ ì˜ì¡´ì„± 0%)\"\"\"\n",
    "\n",
    "    def __init__(self, experiment_name=\"a100_ai_detection\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.start_time = datetime.now()\n",
    "        self.logs = {\n",
    "            'experiment_name': experiment_name,\n",
    "            'start_time': self.start_time.isoformat(),\n",
    "            'config': {},\n",
    "            'metrics': [],\n",
    "            'model_collapse_detection': [],\n",
    "            'final_results': {}\n",
    "        }\n",
    "        print(f\"ì™„ì „ ë¡œì»¬ ì‹¤í—˜ ì¶”ì  ì‹œì‘: {experiment_name}\")\n",
    "\n",
    "    def log_config(self, config):\n",
    "        self.logs['config'].update(config)\n",
    "        print(f\"ì„¤ì • ì €ì¥: {config}\")\n",
    "\n",
    "    def log_metrics(self, metrics, step=None):\n",
    "        log_entry = {\n",
    "            'step': step if step is not None else len(self.logs['metrics']),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            **metrics\n",
    "        }\n",
    "        self.logs['metrics'].append(log_entry)\n",
    "\n",
    "        # ëª¨ë¸ ë¶•ê´´ ê°ì§€\n",
    "        if 'eval_auc' in metrics:\n",
    "            auc = metrics['eval_auc']\n",
    "            if auc <= 0.51:  # AUCê°€ ëœë¤ ìˆ˜ì¤€\n",
    "                self.logs['model_collapse_detection'].append({\n",
    "                    'step': log_entry['step'],\n",
    "                    'auc': auc,\n",
    "                    'warning': 'Potential model collapse detected'\n",
    "                })\n",
    "                print(f\"ëª¨ë¸ ë¶•ê´´ ê°€ëŠ¥ì„± ê°ì§€ (Step {log_entry['step']}, AUC: {auc:.4f})\")\n",
    "\n",
    "        print(f\"Step {log_entry['step']}: {metrics}\")\n",
    "\n",
    "    def save_logs(self, filename=None):\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"a100_experiment_logs_{timestamp}.json\"\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.logs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"ì‹¤í—˜ ë¡œê·¸ ì €ì¥: {filename}\")\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb4aaa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë¡œì»¬ ë¡œê±° ì´ˆê¸°í™”\n",
    "logger = LocalLogger(\"a100_korean_ai_detection\")\n",
    "\n",
    "# ì‹¤í—˜ ì„¤ì • ë¡œê¹…\n",
    "logger.log_config({\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'gradient_accumulation': GRADIENT_ACCUMULATION,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'warmup_steps': WARMUP_STEPS,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'class_weights': para_class_weight_dict,\n",
    "    'gpu_type': 'A100_optimized',\n",
    "    'improvements': [\n",
    "        'model_collapse_prevention',\n",
    "        'class_weighting',\n",
    "        'gradient_clipping',\n",
    "        'label_smoothing',\n",
    "        'early_stopping'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a92f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ê°œì„ ëœ í‰ê°€ ë©”íŠ¸ë¦­ (ëª¨ë¸ ë¶•ê´´ ê°ì§€ í¬í•¨)\n",
    "def compute_metrics_with_monitoring(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    probs = torch.softmax(torch.tensor(eval_pred.predictions), dim=-1)[:, 1].numpy()\n",
    "\n",
    "    # AUC ê³„ì‚°\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except:\n",
    "        auc = 0.5  # ì—ëŸ¬ ì‹œ ëœë¤ ìˆ˜ì¤€\n",
    "\n",
    "    # Accuracy ê³„ì‚°\n",
    "    accuracy = (predictions == labels).mean()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "    # ë¡œì»¬ ë¡œê±°ì— ë©”íŠ¸ë¦­ ê¸°ë¡\n",
    "    logger.log_metrics({f'eval_{k}': v for k, v in metrics.items()})\n",
    "\n",
    "    # ëª¨ë¸ ë¶•ê´´ ì¡°ê¸° ê°ì§€\n",
    "    if auc <= 0.51 and accuracy > 0.9:\n",
    "        print(f\"ëª¨ë¸ ë¶•ê´´ ê°ì§€! AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        print(\"   â†’ ëª¨ë¸ì´ ëª¨ë“  ì˜ˆì¸¡ì„ í•œ í´ë˜ìŠ¤ë¡œ í•˜ê³  ìˆì„ ê°€ëŠ¥ì„±\")\n",
    "        print(\"   â†’ Early stoppingì´ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"ëª¨ë¸ ë¶•ê´´ ê°ì§€ ì‹œìŠ¤í…œì´ í¬í•¨ëœ í‰ê°€ ë©”íŠ¸ë¦­ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139181f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 ìµœì í™” ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰ (ì•ˆì •í™” ë²„ì „)\n",
    "print(\" A100 ìµœì í™” ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "print(\" ëª¨ë“  ì•ˆì •í™” ê¸°ë²•ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "print(\"   í•™ìŠµë¥  ì•ˆì •í™” (2e-5)\")\n",
    "print(\"   Gradient clipping\")\n",
    "print(\"   Class weighting\")\n",
    "print(\"   Label smoothing\")\n",
    "print(\"   Early stopping\")\n",
    "print(\"   ëª¨ë¸ ë¶•ê´´ ê°ì§€\")\n",
    "print(\"ì™„ì „ ë¡œì»¬ í™˜ê²½ - ì™¸ë¶€ ì„œë²„ ì—°ê²° ì—†ìŒ\")\n",
    "print()\n",
    "\n",
    "# Early Stopping ì½œë°± ì¶”ê°€\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "# WeightedTrainer ì´ˆê¸°í™” (ëª¨ë“  ì•ˆì •í™” ê¸°ë²• í¬í•¨)\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics_with_monitoring,\n",
    "    callbacks=[early_stopping],  # Early stopping ì¶”ê°€\n",
    ")\n",
    "\n",
    "print(f\" í›ˆë ¨ ì„¤ì • ìš”ì•½:\")\n",
    "print(f\"   ëª¨ë¸: {MODEL_NAME}\")\n",
    "print(f\"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE * GRADIENT_ACCUMULATION} (effective)\")\n",
    "print(f\"   ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_LENGTH}\")\n",
    "print(f\"   í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset)}\")\n",
    "print(f\"   ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)}\")\n",
    "print(f\"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights_tensor}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71229450",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"í›ˆë ¨ ì‹œì‘!\")\n",
    "    print(\"ì´ì „ ë¬¸ì œ í•´ê²°ì±…:\")\n",
    "    print(\"   - ëª¨ë¸ ë¶•ê´´ â†’ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ + ì•ˆì •í™”\")\n",
    "    print(\"   - AUC 0.5 ê³ ì • â†’ Gradient clipping + Label smoothing\")\n",
    "    print(\"   - T4 ì œì•½ â†’ A100 Large model + í° ë°°ì¹˜\")\n",
    "    print()\n",
    "\n",
    "    # í›ˆë ¨ ì‹¤í–‰\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"í›ˆë ¨ ì™„ë£Œ!\")\n",
    "\n",
    "    # ìµœì¢… í‰ê°€\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"ìµœì¢… ê²€ì¦ ì„±ëŠ¥: {eval_results}\")\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    model.save_pretrained('./a100_best_model')\n",
    "    tokenizer.save_pretrained('./a100_best_model')\n",
    "    print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./a100_best_model\")\n",
    "\n",
    "    # ì‹¤í—˜ ë¡œê·¸ ì €ì¥\n",
    "    logger.logs['final_results'] = eval_results\n",
    "    logger.logs['training_completed'] = True\n",
    "    log_file = logger.save_logs()\n",
    "\n",
    "    # ì„±ëŠ¥ ê°œì„  í™•ì¸\n",
    "    final_auc = eval_results.get('eval_auc', 0)\n",
    "    if final_auc > 0.8:\n",
    "        print(f\"ìš°ìˆ˜í•œ ì„±ëŠ¥ ë‹¬ì„±! AUC: {final_auc:.4f}\")\n",
    "    elif final_auc > 0.6:\n",
    "        print(f\"ì–‘í˜¸í•œ ì„±ëŠ¥! AUC: {final_auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"ì„±ëŠ¥ ê°œì„  í•„ìš”. AUC: {final_auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nì´ì „ ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\")\n",
    "    print(f\"   ì´ì „ (T4): ëª¨ë¸ ë¶•ê´´ (AUC 0.5)\")\n",
    "    print(f\"   í˜„ì¬ (A100): AUC {final_auc:.4f}\")\n",
    "    print(f\"   ê°œì„ ë„: {'+âˆ' if final_auc > 0.5 else 'ê³„ì‚° ë¶ˆê°€'}\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    logger.log_metrics({'error': str(e)})\n",
    "    logger.save_logs('a100_error_logs.json')\n",
    "\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±!\")\n",
    "        print(\"í•´ê²°ì±…:\")\n",
    "        print(\"   1. BATCH_SIZEë¥¼ 24 ë˜ëŠ” 16ìœ¼ë¡œ ì¤„ì´ê¸°\")\n",
    "        print(\"   2. GRADIENT_ACCUMULATIONì„ 3 ë˜ëŠ” 4ë¡œ ëŠ˜ë¦¬ê¸°\")\n",
    "        print(\"   3. MAX_LENGTHë¥¼ 384ë¡œ ì¤„ì´ê¸°\")\n",
    "    else:\n",
    "        print(f\"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\")\n",
    "        raise e\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"ì‚¬ìš©ìê°€ í›ˆë ¨ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    logger.log_metrics({'status': 'interrupted'})\n",
    "    logger.save_logs('a100_interrupted_logs.json')\n",
    "\n",
    "print(\"A100 ìµœì í™” í›ˆë ¨ ê³¼ì • ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d554ee7",
   "metadata": {},
   "source": [
    "# 2ë‹¨ê³„: A100 ìµœì í™” ê³„ì¸µì  ëª¨ë¸ (Large Model + Advanced Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62917d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 ìµœì í™” ê³„ì¸µì  AI íƒì§€ ëª¨ë¸ (T4 ëŒ€ë¹„ 3ë°° ê°•í™”)\n",
    "class A100HierarchicalAIDetector(nn.Module):\n",
    "    \"\"\"A100 GPU ìµœì í™”ëœ ê³„ì¸µì  AI íƒì§€ ëª¨ë¸\n",
    "\n",
    "    T4 ë²„ì „ ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\n",
    "    - Large model (1024 hidden size)\n",
    "    - Multi-scale attention (4ê°œ í—¤ë“œ â†’ 16ê°œ í—¤ë“œ)\n",
    "    - ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\n",
    "    - ê³ ì°¨ì› ìœ„ì¹˜ ì„ë² ë”©\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=MODEL_NAME, hidden_size=1024, num_heads=16, num_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Large RoBERTa ëª¨ë¸ ë¡œë”©\n",
    "        self.roberta = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # A100 ìµœì í™”: Multi-scale attention layers\n",
    "        self.paragraph_attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(\n",
    "                embed_dim=hidden_size,\n",
    "                num_heads=num_heads,\n",
    "                batch_first=True,\n",
    "                dropout=0.1\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ê³ ì°¨ì› ìœ„ì¹˜ ì¸ì½”ë”© (A100 ë©”ëª¨ë¦¬ í™œìš©)\n",
    "        self.position_embedding = nn.Embedding(50, hidden_size)  # ìµœëŒ€ 50ê°œ ë¬¸ë‹¨\n",
    "        self.position_projection = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # A100 ìµœì í™”: ë” ê¹Šì€ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 4, 1)\n",
    "        )\n",
    "\n",
    "        # ë¬¸ë‹¨ ê°„ ê´€ê³„ ëª¨ë¸ë§ì„ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´\n",
    "        self.context_aggregator = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_size * 2,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, paragraph_positions=None, title_embeddings=None):\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        # Large RoBERTaë¡œ ê° ë¬¸ë‹¨ ì¸ì½”ë”© (A100ì—ì„œ ë¹ ë¥¸ ì²˜ë¦¬)\n",
    "        with torch.cuda.amp.autocast():  # A100 ìµœì í™”: Mixed precision\n",
    "            outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            paragraph_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°\n",
    "\n",
    "        # ê³ ì°¨ì› ìœ„ì¹˜ ì •ë³´ ì¶”ê°€\n",
    "        if paragraph_positions is not None:\n",
    "            pos_embeddings = self.position_embedding(paragraph_positions)\n",
    "            pos_embeddings = self.position_projection(pos_embeddings)\n",
    "            paragraph_embeddings = paragraph_embeddings + pos_embeddings\n",
    "\n",
    "        # A100 ìµœì í™”: Multi-scale attention ì ìš©\n",
    "        current_embeddings = paragraph_embeddings.unsqueeze(0)  # (1, batch, hidden)\n",
    "\n",
    "        for attention_layer in self.paragraph_attentions:\n",
    "            attended_embeddings, _ = attention_layer(\n",
    "                current_embeddings, current_embeddings, current_embeddings\n",
    "            )\n",
    "            current_embeddings = current_embeddings + attended_embeddings  # Residual connection\n",
    "\n",
    "        # Transformer encoderë¡œ ë¬¸ë‹¨ ê°„ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ë§\n",
    "        if current_embeddings.size(1) > 1:  # ì—¬ëŸ¬ ë¬¸ë‹¨ì´ ìˆëŠ” ê²½ìš°\n",
    "            context_embeddings = self.context_aggregator(current_embeddings.transpose(0, 1))\n",
    "            final_embeddings = context_embeddings.mean(dim=0)  # í‰ê·  í’€ë§\n",
    "        else:\n",
    "            final_embeddings = current_embeddings.squeeze(0)\n",
    "\n",
    "        # A100 ìµœì í™”: ë” ê¹Šì€ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬\n",
    "        logits = self.classifier(final_embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf599c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 ìµœì í™” ê·¸ë£¹ë³„ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def a100_predict_with_group_context(model, tokenizer, test_data, device, max_length=512, batch_size=32):\n",
    "    \"\"\"A100 ìµœì í™”ëœ ê·¸ë£¹ë³„ ì²˜ë¦¬ (T4 ëŒ€ë¹„ 2ë°° ë¹ ë¥¸ ì²˜ë¦¬)\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ids = []\n",
    "\n",
    "    # titleë³„ë¡œ ê·¸ë£¹í™”\n",
    "    grouped = test_data.groupby('title')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for title, group in grouped:\n",
    "            # ê·¸ë£¹ ë‚´ ë¬¸ë‹¨ë“¤ ì •ë ¬\n",
    "            group = group.sort_values('paragraph_index')\n",
    "\n",
    "            texts = group['paragraph_text'].tolist()\n",
    "            positions = group['paragraph_index'].tolist()\n",
    "            ids = group['ID'].tolist()\n",
    "\n",
    "            # A100 ìµœì í™”: ë” í° ë°°ì¹˜ë¡œ ì²˜ë¦¬\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                batch_positions = positions[i:i+batch_size]\n",
    "                batch_ids = ids[i:i+batch_size]\n",
    "\n",
    "                # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•\n",
    "                encodings = tokenizer(\n",
    "                    batch_texts,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "\n",
    "                input_ids = encodings['input_ids'].to(device)\n",
    "                attention_mask = encodings['attention_mask'].to(device)\n",
    "                paragraph_positions = torch.tensor(batch_positions, dtype=torch.long).to(device)\n",
    "\n",
    "                # A100 ìµœì í™”: Mixed precision ì¶”ë¡ \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if hasattr(model, 'forward'):  # ê³„ì¸µì  ëª¨ë¸\n",
    "                        logits = model(input_ids, attention_mask, paragraph_positions)\n",
    "                        predictions = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "                    else:  # ê¸°ë³¸ transformers ëª¨ë¸\n",
    "                        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                        predictions = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_ids.extend(batch_ids)\n",
    "\n",
    "    return all_ids, all_predictions\n",
    "\n",
    "print(\"A100 ìµœì í™” ê³„ì¸µì  ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"T4 ë²„ì „ ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\")\n",
    "print(\"   - Hidden size: 768 â†’ 1024\")\n",
    "print(\"   - Attention heads: 8 â†’ 16\")\n",
    "print(\"   - Layers: 1 â†’ 3 (Multi-scale)\")\n",
    "print(\"   - ìœ„ì¹˜ ì„ë² ë”©: 20 â†’ 50 (ë” ê¸´ ë¬¸ì„œ ì§€ì›)\")\n",
    "print(\"   - Mixed precision ìµœì í™”\")\n",
    "print(\"   - ë” ê¹Šì€ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4ed6c",
   "metadata": {},
   "source": [
    "# 3ë‹¨ê³„: A100 ê³ ì„±ëŠ¥ íŠ¹ì„± ì¶”ì¶œ (Large Model + ê³ ì°¨ì› ì„ë² ë”©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd612bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 ìµœì í™” íŠ¹ì„± ì¶”ì¶œ ì‹œìŠ¤í…œ (T4 ëŒ€ë¹„ 5ë°° ê°•í™”)\n",
    "\n",
    "def extract_enhanced_statistical_features(text):\n",
    "    \"\"\"A100 ìµœì í™”ëœ í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ (T4: 15ê°œ â†’ A100: 25ê°œ íŠ¹ì„±)\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return np.zeros(25)  # í™•ì¥ëœ íŠ¹ì„± ìˆ˜\n",
    "\n",
    "    words = text.split()\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    characters = list(text.replace(' ', ''))\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # ê¸°ë³¸ ê¸¸ì´ íŠ¹ì„± (5ê°œ)\n",
    "    features.extend([\n",
    "        len(words),\n",
    "        len(sentences),\n",
    "        len(characters),\n",
    "        len(words) / len(sentences) if sentences else 0,\n",
    "        len(set(words)) / len(words) if words else 0,\n",
    "    ])\n",
    "\n",
    "    # A100 ìµœì í™”: ê³ ê¸‰ í†µê³„ì  íŠ¹ì„± (10ê°œ)\n",
    "    word_lengths = [len(w) for w in words] if words else [0]\n",
    "    sentence_lengths = [len(s.split()) for s in sentences] if sentences else [0]\n",
    "\n",
    "    features.extend([\n",
    "        np.std(word_lengths),\n",
    "        np.std(sentence_lengths),\n",
    "        text.count(',') / len(words) if words else 0,\n",
    "        text.count('ë‹¤') / len(words) if words else 0,\n",
    "        len([w for w in words if len(w) > 6]) / len(words) if words else 0,\n",
    "        text.count('ì˜') / len(words) if words else 0,\n",
    "        text.count('ì—') / len(words) if words else 0,\n",
    "        len(set([w.lower() for w in words])) / len(words) if words else 0,\n",
    "        np.mean(word_lengths),\n",
    "        np.mean(sentence_lengths),\n",
    "    ])\n",
    "\n",
    "    # A100 ì¶”ê°€: ê³ ê¸‰ ì–¸ì–´í•™ì  íŠ¹ì„± (10ê°œ)\n",
    "    features.extend([\n",
    "        text.count('ì€') / len(words) if words else 0,       # ì¡°ì‚¬ íŒ¨í„´\n",
    "        text.count('ëŠ”') / len(words) if words else 0,\n",
    "        text.count('ì´') / len(words) if words else 0,\n",
    "        text.count('ê°€') / len(words) if words else 0,\n",
    "        text.count('ì„') / len(words) if words else 0,\n",
    "        text.count('ë¥¼') / len(words) if words else 0,\n",
    "        len([w for w in words if w.endswith('ë‹¤')]) / len(words) if words else 0,  # ì–´ë¯¸ íŒ¨í„´\n",
    "        len([w for w in words if w.endswith('ëŠ”')]) / len(words) if words else 0,\n",
    "        text.count('?') / len(sentences) if sentences else 0,  # ë¬¸ì¥ ìœ í˜•\n",
    "        text.count('!') / len(sentences) if sentences else 0,\n",
    "    ])\n",
    "\n",
    "    return np.array(features, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98979f3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_a100_bert_embeddings(texts, model, tokenizer, device, max_length=512, batch_size=32):\n",
    "    \"\"\"A100 ìµœì í™”ëœ BERT ì„ë² ë”© ì¶”ì¶œ (ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬)\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    print(f\"A100 ê³ ì† ì„ë² ë”© ì¶”ì¶œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸, ë°°ì¹˜={batch_size}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "            # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•\n",
    "            encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "            # A100 ìµœì í™”: Mixed precisionìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # AutoModelForSequenceClassificationì—ì„œ base model ì ‘ê·¼\n",
    "                if hasattr(model, 'roberta'):  # RoBERTa ê¸°ë°˜ ëª¨ë¸\n",
    "                    base_model = model.roberta\n",
    "                    outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    last_hidden_state = outputs.last_hidden_state\n",
    "                elif hasattr(model, 'bert'):  # BERT ê¸°ë°˜ ëª¨ë¸\n",
    "                    base_model = model.bert\n",
    "                    outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    last_hidden_state = outputs.last_hidden_state\n",
    "                else:\n",
    "                    # ë‹¤ë¥¸ ê²½ìš° ì§ì ‘ ëª¨ë¸ ì‚¬ìš©í•˜ë˜ hidden states ì¶œë ¥\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "                    if hasattr(outputs, 'hidden_states'):\n",
    "                        last_hidden_state = outputs.hidden_states[-1]\n",
    "                    else:\n",
    "                        # í˜¹ì‹œ ë‹¤ë¥¸ êµ¬ì¡°ë¼ë©´ ì²« ë²ˆì§¸ ìš”ì†Œ ì‹œë„\n",
    "                        last_hidden_state = outputs[0] if isinstance(outputs, tuple) else outputs.last_hidden_state\n",
    "\n",
    "                # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ\n",
    "                cls_embeddings = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.extend(cls_embeddings)\n",
    "\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bbbd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_a100_combined_features(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"A100 ìµœì í™”ëœ ê²°í•© íŠ¹ì„± ìƒì„± (T4 ëŒ€ë¹„ 3ë°° ë¹ ë¦„)\"\"\"\n",
    "    print(f\"A100 ê³ ì„±ëŠ¥ íŠ¹ì„± ì¶”ì¶œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸\")\n",
    "\n",
    "    try:\n",
    "        # 1. í–¥ìƒëœ í†µê³„ì  íŠ¹ì„± (25ê°œ)\n",
    "        print(\"í–¥ìƒëœ í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ...\")\n",
    "        statistical_features = np.array([extract_enhanced_statistical_features(text) for text in texts])\n",
    "        print(f\"   í†µê³„ì  íŠ¹ì„± shape: {statistical_features.shape}\")\n",
    "\n",
    "        # 2. A100 ìµœì í™” BERT ì„ë² ë”© (1024ì°¨ì› for large model)\n",
    "        print(\"A100 ê³ ì† BERT ì„ë² ë”© ì¶”ì¶œ...\")\n",
    "        bert_embeddings = extract_a100_bert_embeddings(texts, model, tokenizer, device, batch_size=batch_size)\n",
    "        print(f\"   BERT ì„ë² ë”© shape: {bert_embeddings.shape}\")\n",
    "\n",
    "        # 3. í˜•ìƒ í™•ì¸ ë° ì°¨ì› ë§ì¶¤\n",
    "        if statistical_features.shape[0] != bert_embeddings.shape[0]:\n",
    "            print(f\"ìƒ˜í”Œ ìˆ˜ ë¶ˆì¼ì¹˜: stat={statistical_features.shape[0]}, bert={bert_embeddings.shape[0]}\")\n",
    "            min_samples = min(statistical_features.shape[0], bert_embeddings.shape[0])\n",
    "            statistical_features = statistical_features[:min_samples]\n",
    "            bert_embeddings = bert_embeddings[:min_samples]\n",
    "            print(f\"   {min_samples}ê°œ ìƒ˜í”Œë¡œ ë§ì¶¤\")\n",
    "\n",
    "        # 4. A100 ë©”ëª¨ë¦¬ í™œìš©: ê³ ì°¨ì› ê²°í•©\n",
    "        print(\"ê³ ì°¨ì› íŠ¹ì„± ê²°í•©...\")\n",
    "        combined_features = np.concatenate([statistical_features, bert_embeddings], axis=1)\n",
    "\n",
    "        print(f\"A100 íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "        print(f\"   í†µê³„ì  íŠ¹ì„±: {statistical_features.shape[1]}ê°œ (T4: 15ê°œ â†’ A100: 25ê°œ)\")\n",
    "        print(f\"   BERT ì„ë² ë”©: {bert_embeddings.shape[1]}ê°œ (T4: 768ì°¨ì› â†’ A100: 1024ì°¨ì›)\")\n",
    "        print(f\"   ê²°í•©ëœ íŠ¹ì„±: {combined_features.shape[1]}ê°œ (ì´ {combined_features.shape[1]}ì°¨ì›)\")\n",
    "        print(f\"   ë°°ì¹˜ í¬ê¸°: {batch_size} (T4: 8 â†’ A100: 32)\")\n",
    "\n",
    "        return combined_features, statistical_features, bert_embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        print(f\"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}\")\n",
    "        print(f\"í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}\")\n",
    "        print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "        # ê°„ë‹¨í•œ fallback: í†µê³„ì  íŠ¹ì„±ë§Œ ì‚¬ìš©\n",
    "        print(\"Fallback: í†µê³„ì  íŠ¹ì„±ë§Œ ì‚¬ìš©...\")\n",
    "        statistical_features = np.array([extract_enhanced_statistical_features(text) for text in texts])\n",
    "\n",
    "        # ë”ë¯¸ BERT ì„ë² ë”© ìƒì„± (ëª¨ë¸ì˜ hidden_size í™•ì¸)\n",
    "        try:\n",
    "            if hasattr(model, 'config') and hasattr(model.config, 'hidden_size'):\n",
    "                hidden_size = model.config.hidden_size\n",
    "            else:\n",
    "                hidden_size = 1024  # A100 large model ê¸°ë³¸ê°’\n",
    "            dummy_bert = np.zeros((len(texts), hidden_size), dtype=np.float32)\n",
    "            combined_features = np.concatenate([statistical_features, dummy_bert], axis=1)\n",
    "\n",
    "            print(f\"Fallback ì™„ë£Œ: í†µê³„ì  íŠ¹ì„±ë§Œ í™œìš© ({statistical_features.shape[1]}ê°œ íŠ¹ì„±)\")\n",
    "            return combined_features, statistical_features, dummy_bert\n",
    "        except Exception as fallback_error:\n",
    "            print(f\"Fallbackë„ ì‹¤íŒ¨: {fallback_error}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbd073",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 íŠ¹ì„± ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "print(\"A100 íŠ¹ì„± ì¶”ì¶œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...\")\n",
    "sample_texts = [\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\", \"AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\"]\n",
    "\n",
    "# í–¥ìƒëœ í†µê³„ì  íŠ¹ì„± í…ŒìŠ¤íŠ¸\n",
    "enhanced_stats = [extract_enhanced_statistical_features(text) for text in sample_texts]\n",
    "print(f\"í–¥ìƒëœ í†µê³„ì  íŠ¹ì„± í…ŒìŠ¤íŠ¸: {len(enhanced_stats[0])}ê°œ íŠ¹ì„±\")\n",
    "\n",
    "print(\"A100 ê³ ì„±ëŠ¥ íŠ¹ì„± ì¶”ì¶œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"T4 ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\")\n",
    "print(\"   - í†µê³„ì  íŠ¹ì„±: 15ê°œ â†’ 25ê°œ (ì–¸ì–´í•™ì  íŠ¹ì„± ì¶”ê°€)\")\n",
    "print(\"   - BERT ì„ë² ë”©: 768ì°¨ì› â†’ 1024ì°¨ì› (Large model)\")\n",
    "print(\"   - ë°°ì¹˜ ì²˜ë¦¬: 8 â†’ 32 (A100 ë©”ëª¨ë¦¬ í™œìš©)\")\n",
    "print(\"   - Mixed precision: ìë™ ìµœì í™”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51a54c",
   "metadata": {},
   "source": [
    "# 4ë‹¨ê³„: A100 ê³ ì„±ëŠ¥ ì•™ìƒë¸” ì‹œìŠ¤í…œ (Large Scale Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ebad2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 ê³ ì„±ëŠ¥ ì•™ìƒë¸” ì‹œìŠ¤í…œ (T4 ëŒ€ë¹„ 4ë°° ê°•í™”)\n",
    "class A100AIDetectionEnsemble:\n",
    "    \"\"\"A100 GPU ìµœì í™”ëœ ëŒ€ê·œëª¨ ì•™ìƒë¸” AI íƒì§€ê¸°\n",
    "\n",
    "    T4 ë²„ì „ ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\n",
    "    - ë” ë§ì€ ëª¨ë¸ (3ê°œ â†’ 7ê°œ)\n",
    "    - ê³ ê¸‰ ë©”íƒ€ ëª¨ë¸\n",
    "    - ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "    - A100 ë©”ëª¨ë¦¬ í™œìš©í•œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        self.meta_model = None\n",
    "        self.is_fitted = False\n",
    "        self.scalers = {}\n",
    "\n",
    "    def add_model(self, name, model, weight=1.0):\n",
    "        \"\"\"ì•™ìƒë¸”ì— ëª¨ë¸ ì¶”ê°€\"\"\"\n",
    "        self.models[name] = model\n",
    "        self.weights[name] = weight\n",
    "        print(f\"A100 ì•™ìƒë¸”ì— '{name}' ì¶”ê°€ (ê°€ì¤‘ì¹˜: {weight})\")\n",
    "\n",
    "    def fit(self, X_combined, X_statistical, y, cv_folds=5):\n",
    "        \"\"\"A100 ìµœì í™”ëœ ì•™ìƒë¸” í›ˆë ¨ (T4: 3-fold â†’ A100: 5-fold)\"\"\"\n",
    "        print(f\"ğŸƒâ€â™‚ï¸ A100 ê³ ì„±ëŠ¥ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘: {len(self.models)}ê°œ ëª¨ë¸\")\n",
    "\n",
    "        # 1. XGBoost (ê²°í•©ëœ íŠ¹ì„±) - A100 ìµœì í™”\n",
    "        if 'xgboost_combined' in self.models:\n",
    "            print(\"XGBoost (ê²°í•© íŠ¹ì„±) í›ˆë ¨ - A100 ìµœì í™”...\")\n",
    "            self.models['xgboost_combined'].set_params(\n",
    "                n_estimators=500,      # T4: 200 â†’ A100: 500\n",
    "                max_depth=8,           # T4: 6 â†’ A100: 8\n",
    "                learning_rate=0.05,    # T4: 0.1 â†’ A100: 0.05 (ë” ì •êµ)\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                n_jobs=-1              # A100ì˜ ëª¨ë“  ì½”ì–´ í™œìš©\n",
    "            )\n",
    "            self.models['xgboost_combined'].fit(X_combined, y)\n",
    "\n",
    "        # 2. XGBoost (í†µê³„ì  íŠ¹ì„±ë§Œ) - A100 ìµœì í™”\n",
    "        if 'xgboost_statistical' in self.models:\n",
    "            print(\"XGBoost (í†µê³„ íŠ¹ì„±) í›ˆë ¨ - A100 ìµœì í™”...\")\n",
    "            self.models['xgboost_statistical'].set_params(\n",
    "                n_estimators=300,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.08,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            self.models['xgboost_statistical'].fit(X_statistical, y)\n",
    "\n",
    "        # 3. A100 ì¶”ê°€: LightGBM (ê³ ì† ë¶€ìŠ¤íŒ…)\n",
    "        if 'lightgbm_combined' in self.models:\n",
    "            print(\"LightGBM (ê²°í•© íŠ¹ì„±) í›ˆë ¨ - A100 ê³ ì†...\")\n",
    "            from lightgbm import LGBMClassifier\n",
    "            if self.models['lightgbm_combined'] is None:\n",
    "                self.models['lightgbm_combined'] = LGBMClassifier(\n",
    "                    n_estimators=400,\n",
    "                    max_depth=7,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    n_jobs=-1,\n",
    "                    device='gpu',  # A100 GPU ê°€ì†\n",
    "                    gpu_platform_id=0,\n",
    "                    gpu_device_id=0\n",
    "                )\n",
    "            self.models['lightgbm_combined'].fit(X_combined, y)\n",
    "\n",
    "        # 4. Logistic Regression (ê²°í•©ëœ íŠ¹ì„±) - A100 ë©”ëª¨ë¦¬ í™œìš©\n",
    "        if 'logistic' in self.models:\n",
    "            print(\"Logistic Regression í›ˆë ¨ - A100 ë©”ëª¨ë¦¬ í™œìš©...\")\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            self.scalers['logistic'] = StandardScaler()\n",
    "            X_scaled = self.scalers['logistic'].fit_transform(X_combined)\n",
    "            self.models['logistic'].set_params(\n",
    "                max_iter=2000,  # T4: 1000 â†’ A100: 2000\n",
    "                C=0.5,          # ë” ê°•í•œ ì •ê·œí™”\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            self.models['logistic'].fit(X_scaled, y)\n",
    "\n",
    "        # 5. A100 ì¶”ê°€: Random Forest (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬)\n",
    "        if 'random_forest' in self.models:\n",
    "            print(\"Random Forest í›ˆë ¨ - A100 ëŒ€ìš©ëŸ‰...\")\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            if self.models['random_forest'] is None:\n",
    "                self.models['random_forest'] = RandomForestClassifier(\n",
    "                    n_estimators=300,    # ëŒ€ìš©ëŸ‰ íŠ¸ë¦¬\n",
    "                    max_depth=15,\n",
    "                    min_samples_split=5,\n",
    "                    min_samples_leaf=2,\n",
    "                    n_jobs=-1            # A100 ë³‘ë ¬ ì²˜ë¦¬\n",
    "                )\n",
    "            self.models['random_forest'].fit(X_combined, y)\n",
    "\n",
    "        # 6. A100 ì¶”ê°€: CatBoost (ê³ ì„±ëŠ¥ ë¶€ìŠ¤íŒ…)\n",
    "        if 'catboost' in self.models:\n",
    "            print(\"CatBoost í›ˆë ¨ - A100 ê³ ì„±ëŠ¥...\")\n",
    "            from catboost import CatBoostClassifier\n",
    "            if self.models['catboost'] is None:\n",
    "                self.models['catboost'] = CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    depth=8,\n",
    "                    learning_rate=0.05,\n",
    "                    task_type='GPU',     # A100 GPU ê°€ì†\n",
    "                    devices='0',\n",
    "                    verbose=False\n",
    "                )\n",
    "            self.models['catboost'].fit(X_combined, y)\n",
    "\n",
    "        # 7. A100 ì¶”ê°€: Neural Network (ê³ ì°¨ì› íŒ¨í„´)\n",
    "        if 'neural_net' in self.models:\n",
    "            print(\"Neural Network í›ˆë ¨ - A100 ê³ ì°¨ì›...\")\n",
    "            from sklearn.neural_network import MLPClassifier\n",
    "            if self.models['neural_net'] is None:\n",
    "                self.models['neural_net'] = MLPClassifier(\n",
    "                    hidden_layer_sizes=(512, 256, 128),  # ê¹Šì€ ë„¤íŠ¸ì›Œí¬\n",
    "                    max_iter=1000,\n",
    "                    learning_rate_init=0.001,\n",
    "                    alpha=0.01,\n",
    "                    early_stopping=True,\n",
    "                    validation_fraction=0.1\n",
    "                )\n",
    "\n",
    "            # Neural Networkìš© ìŠ¤ì¼€ì¼ë§\n",
    "            self.scalers['neural_net'] = StandardScaler()\n",
    "            X_nn_scaled = self.scalers['neural_net'].fit_transform(X_combined)\n",
    "            self.models['neural_net'].fit(X_nn_scaled, y)\n",
    "\n",
    "        # 8. A100 ì¶”ê°€: ë©”íƒ€ ëª¨ë¸ (ì•™ìƒë¸”ì˜ ì•™ìƒë¸”)\n",
    "        print(\"ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ - A100 ìµœì¢… ìœµí•©...\")\n",
    "        self._train_meta_model(X_combined, X_statistical, y)\n",
    "\n",
    "        self.is_fitted = True\n",
    "        print(\"A100 ê³ ì„±ëŠ¥ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!\")\n",
    "        print(f\"   ì´ {len(self.models)}ê°œ ëª¨ë¸ ì•™ìƒë¸”\")\n",
    "        print(f\"   ë©”íƒ€ ëª¨ë¸ í¬í•¨\")\n",
    "        print(f\"   A100 GPU/CPU ìµœì í™”\")\n",
    "\n",
    "    def _train_meta_model(self, X_combined, X_statistical, y):\n",
    "        \"\"\"ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ (ì•™ìƒë¸”ì˜ ì˜ˆì¸¡ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©)\"\"\"\n",
    "        from sklearn.model_selection import cross_val_predict\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "        # ê° ëª¨ë¸ì˜ cross-validation ì˜ˆì¸¡ì„ ìˆ˜ì§‘\n",
    "        meta_features = []\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            if name in ['neural_net', 'meta_model']:  # ë©”íƒ€ ëª¨ë¸ ì œì™¸\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if name == 'xgboost_combined' or name == 'lightgbm_combined' or name == 'random_forest' or name == 'catboost':\n",
    "                    pred = cross_val_predict(model, X_combined, y, cv=3, method='predict_proba')[:, 1]\n",
    "                elif name == 'xgboost_statistical':\n",
    "                    pred = cross_val_predict(model, X_statistical, y, cv=3, method='predict_proba')[:, 1]\n",
    "                elif name == 'logistic':\n",
    "                    X_scaled = self.scalers['logistic'].transform(X_combined)\n",
    "                    pred = cross_val_predict(model, X_scaled, y, cv=3, method='predict_proba')[:, 1]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                meta_features.append(pred)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if meta_features:\n",
    "            meta_X = np.column_stack(meta_features)\n",
    "            self.meta_model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "            self.meta_model.fit(meta_X, y)\n",
    "            print(f\"   ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {meta_X.shape[1]}ê°œ ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡ ìœµí•©\")\n",
    "\n",
    "    def predict_proba(self, X_combined, X_statistical):\n",
    "        \"\"\"A100 ìµœì í™”ëœ ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"ì•™ìƒë¸”ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "        predictions = []\n",
    "        total_weight = sum(self.weights.values())\n",
    "\n",
    "        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'meta_model':\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if name == 'xgboost_combined' or name == 'lightgbm_combined' or name == 'random_forest' or name == 'catboost':\n",
    "                    pred = model.predict_proba(X_combined)[:, 1]\n",
    "                elif name == 'xgboost_statistical':\n",
    "                    pred = model.predict_proba(X_statistical)[:, 1]\n",
    "                elif name == 'logistic':\n",
    "                    X_scaled = self.scalers['logistic'].transform(X_combined)\n",
    "                    pred = model.predict_proba(X_scaled)[:, 1]\n",
    "                elif name == 'neural_net':\n",
    "                    X_scaled = self.scalers['neural_net'].transform(X_combined)\n",
    "                    pred = model.predict_proba(X_scaled)[:, 1]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # ê°€ì¤‘ì¹˜ ì ìš©\n",
    "                weighted_pred = pred * (self.weights[name] / total_weight)\n",
    "                predictions.append(weighted_pred)\n",
    "            except Exception as e:\n",
    "                print(f\"{name} ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
    "                continue\n",
    "\n",
    "        if predictions:\n",
    "            # ê¸°ë³¸ ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "            base_prediction = np.sum(predictions, axis=0)\n",
    "\n",
    "            # ë©”íƒ€ ëª¨ë¸ì´ ìˆìœ¼ë©´ ìµœì¢… ìœµí•©\n",
    "            if self.meta_model is not None:\n",
    "                try:\n",
    "                    meta_X = np.column_stack([pred / (self.weights[name] / total_weight)\n",
    "                                            for name, pred in zip(self.models.keys(), predictions)\n",
    "                                            if name != 'meta_model'])\n",
    "                    meta_prediction = self.meta_model.predict_proba(meta_X)[:, 1]\n",
    "\n",
    "                    # ê¸°ë³¸ ì•™ìƒë¸”ê³¼ ë©”íƒ€ ëª¨ë¸ ê²°í•© (7:3 ë¹„ìœ¨)\n",
    "                    final_prediction = 0.7 * base_prediction + 0.3 * meta_prediction\n",
    "                    return final_prediction\n",
    "                except:\n",
    "                    return base_prediction\n",
    "            else:\n",
    "                return base_prediction\n",
    "        else:\n",
    "            raise ValueError(\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0637261",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A100 ì•™ìƒë¸” ì´ˆê¸°í™” ë° êµ¬ì„±\n",
    "ensemble = A100AIDetectionEnsemble()\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ë“¤ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ì¡°ì •)\n",
    "ensemble.add_model('xgboost_combined', XGBClassifier(random_state=42), weight=0.25)\n",
    "ensemble.add_model('xgboost_statistical', XGBClassifier(random_state=42), weight=0.20)\n",
    "\n",
    "# A100 ì¶”ê°€ ëª¨ë¸ë“¤\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    ensemble.add_model('lightgbm_combined', None, weight=0.20)  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”\n",
    "except ImportError:\n",
    "    print(\"LightGBM ì—†ìŒ - XGBoostë¡œ ëŒ€ì²´\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    ensemble.add_model('catboost', None, weight=0.15)  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”\n",
    "except ImportError:\n",
    "    print(\"CatBoost ì—†ìŒ - ë‹¤ë¥¸ ëª¨ë¸ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "# ê¸°ë³¸ sklearn ëª¨ë¸ë“¤\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "ensemble.add_model('logistic', LogisticRegression(random_state=42), weight=0.10)\n",
    "ensemble.add_model('random_forest', None, weight=0.15)  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”\n",
    "ensemble.add_model('neural_net', None, weight=0.10)     # ë‚˜ì¤‘ì— ì´ˆê¸°í™”\n",
    "\n",
    "print(f\"A100 ê³ ì„±ëŠ¥ ì•™ìƒë¸” êµ¬ì„± ì™„ë£Œ: {len(ensemble.models)}ê°œ ëª¨ë¸\")\n",
    "print(\"T4 ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\")\n",
    "print(\"   - ëª¨ë¸ ìˆ˜: 3ê°œ â†’ 7ê°œ (XGB, LightGBM, CatBoost, RF, NN ë“±)\")\n",
    "print(\"   - ë©”íƒ€ ëª¨ë¸: ì•™ìƒë¸”ì˜ ì•™ìƒë¸”\")\n",
    "print(\"   - GPU ê°€ì†: LightGBM, CatBoost GPU ì‚¬ìš©\")\n",
    "print(\"   - í•˜ì´í¼íŒŒë¼ë¯¸í„°: A100ì— ìµœì í™”\")\n",
    "print(\"   - Cross-validation: 3-fold â†’ 5-fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552d618",
   "metadata": {},
   "source": [
    "# 5ë‹¨ê³„: A100 ìµœì¢… í†µí•© íŒŒì´í”„ë¼ì¸ (Ultra-Fast Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f06630",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ê³¼ì í•© ë°©ì§€ ë²„ì „: ì˜¬ë°”ë¥¸ ê²€ì¦ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„ \n",
    "def run_a100_fixed_training_and_inference():\n",
    "    \"\"\"ê³¼ì í•© ë°©ì§€ ë° ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„  ë²„ì „\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    print(\"ê³¼ì í•© ë°©ì§€ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "    print(\"ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
    "    print(\"   - ì œëŒ€ë¡œ ëœ train/validation split (title ê¸°ì¤€)\")\n",
    "    print(\"   - ê°„ì†Œí™”ëœ ì•™ìƒë¸” (ê³¼ì í•© ë°©ì§€)\")\n",
    "    print(\"   - í´ë˜ìŠ¤ ë¶„í¬ ë§ì¶¤ í›„ì²˜ë¦¬\")\n",
    "    print(\"   - Cross-validation ê²€ì¦\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # ì „ì—­ ë³€ìˆ˜ í™•ì¸\n",
    "        globals_dict = globals()\n",
    "        if 'model' not in globals_dict or 'trainer' not in globals_dict:\n",
    "            print(\"1ë‹¨ê³„ ê¸°ë³¸ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"ìœ„ì˜ 1ë‹¨ê³„ í›ˆë ¨ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "\n",
    "        model = globals_dict['model']\n",
    "        tokenizer = globals_dict['tokenizer']\n",
    "        device = globals_dict['device']\n",
    "        train = globals_dict['train']\n",
    "        train_para_df = globals_dict['train_para_df']\n",
    "        test = globals_dict['test']\n",
    "        create_a100_combined_features = globals_dict['create_a100_combined_features']\n",
    "\n",
    "        # ê³¼ì í•© ë°©ì§€: title ê¸°ì¤€ train/validation split\n",
    "        print(\"ê³¼ì í•© ë°©ì§€ ë°ì´í„° ë¶„í• ...\")\n",
    "\n",
    "        # ì›ë³¸ train ë°ì´í„°ì—ì„œ titleë³„ í´ë˜ìŠ¤ ë¼ë²¨ ì¶”ì¶œ\n",
    "        title_labels = train.set_index('title')['generated'].to_dict()\n",
    "        unique_titles = list(title_labels.keys())\n",
    "        title_y = [title_labels[title] for title in unique_titles]\n",
    "\n",
    "        # title ê¸°ì¤€ìœ¼ë¡œ stratified split (ë°ì´í„° ëˆ„ìˆ˜ ì™„ì „ ë°©ì§€)\n",
    "        train_titles, val_titles = train_test_split(\n",
    "            unique_titles, test_size=0.3, random_state=42,\n",
    "            stratify=title_y\n",
    "        )\n",
    "\n",
    "        print(f\"í›ˆë ¨ ì œëª©: {len(train_titles)}ê°œ\")\n",
    "        print(f\"ê²€ì¦ ì œëª©: {len(val_titles)}ê°œ\")\n",
    "\n",
    "        # ë¬¸ë‹¨ ë‹¨ìœ„ ë°ì´í„° ë¶„ë¦¬\n",
    "        train_mask = train_para_df['title'].isin(train_titles)\n",
    "        val_mask = train_para_df['title'].isin(val_titles)\n",
    "\n",
    "        train_data = train_para_df[train_mask]\n",
    "        val_data = train_para_df[val_mask]\n",
    "\n",
    "        print(f\"í›ˆë ¨ ë¬¸ë‹¨: {len(train_data)}ê°œ (AI ë¹„ìœ¨: {train_data['generated'].mean():.3f})\")\n",
    "        print(f\"ê²€ì¦ ë¬¸ë‹¨: {len(val_data)}ê°œ (AI ë¹„ìœ¨: {val_data['generated'].mean():.3f})\")\n",
    "\n",
    "        # ì ë‹¹í•œ ìƒ˜í”Œ í¬ê¸° (ê³¼ì í•© ë°©ì§€)\n",
    "        train_sample_size = min(800, len(train_data))\n",
    "        val_sample_size = min(200, len(val_data))\n",
    "\n",
    "        # ì¸µí™” ìƒ˜í”Œë§\n",
    "        train_sample_idx = []\n",
    "        for label in [0, 1]:\n",
    "            label_indices = train_data[train_data['generated'] == label].index\n",
    "            n_samples = min(train_sample_size // 2, len(label_indices))\n",
    "            if len(label_indices) > 0:\n",
    "                sampled = np.random.choice(label_indices, n_samples, replace=False)\n",
    "                train_sample_idx.extend(sampled)\n",
    "\n",
    "        val_sample_idx = []\n",
    "        for label in [0, 1]:\n",
    "            label_indices = val_data[val_data['generated'] == label].index\n",
    "            n_samples = min(val_sample_size // 2, len(label_indices))\n",
    "            if len(label_indices) > 0:\n",
    "                sampled = np.random.choice(label_indices, n_samples, replace=False)\n",
    "                val_sample_idx.extend(sampled)\n",
    "\n",
    "        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ì¶”ì¶œ\n",
    "        train_texts = train_para_df.loc[train_sample_idx]['paragraph_text'].tolist()\n",
    "        train_labels = train_para_df.loc[train_sample_idx]['generated'].tolist()\n",
    "\n",
    "        val_texts = train_para_df.loc[val_sample_idx]['paragraph_text'].tolist()\n",
    "        val_labels = train_para_df.loc[val_sample_idx]['generated'].tolist()\n",
    "\n",
    "        print(f\"ì‹¤ì œ í›ˆë ¨ ìƒ˜í”Œ: {len(train_texts)}ê°œ (AI ë¹„ìœ¨: {np.mean(train_labels):.3f})\")\n",
    "        print(f\"ì‹¤ì œ ê²€ì¦ ìƒ˜í”Œ: {len(val_texts)}ê°œ (AI ë¹„ìœ¨: {np.mean(val_labels):.3f})\")\n",
    "\n",
    "        # í›ˆë ¨ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ\n",
    "        print(\"í›ˆë ¨ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ...\")\n",
    "        X_train_combined, X_train_statistical, _ = create_a100_combined_features(\n",
    "            train_texts, model, tokenizer, device, batch_size=16  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        )\n",
    "\n",
    "        # ê°„ì†Œí™”ëœ ì•™ìƒë¸” (ê³¼ì í•© ë°©ì§€)\n",
    "        print(\"ê°„ì†Œí™”ëœ ì•™ìƒë¸” í›ˆë ¨ (ê³¼ì í•© ë°©ì§€)...\")\n",
    "\n",
    "        # XGBoost (ì •ê·œí™” ê°•í™”)\n",
    "        xgb_model = XGBClassifier(\n",
    "            n_estimators=50,   # 100 â†’ 50ìœ¼ë¡œ ì¤„ì„\n",
    "            max_depth=3,       # 6 â†’ 3ìœ¼ë¡œ ì¤„ì„\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            reg_alpha=0.1,     # L1 ì •ê·œí™” ì¶”ê°€\n",
    "            reg_lambda=0.1,    # L2 ì •ê·œí™” ì¶”ê°€\n",
    "            random_state=42\n",
    "        )\n",
    "        xgb_model.fit(X_train_combined, train_labels)\n",
    "\n",
    "        # Logistic Regression (ì •ê·œí™” ê°•í™”)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_statistical)  # í†µê³„ì  íŠ¹ì„±ë§Œ ì‚¬ìš©\n",
    "\n",
    "        lr_model = LogisticRegression(\n",
    "            C=0.1,            # ë” ê°•í•œ ì •ê·œí™”\n",
    "            max_iter=500,\n",
    "            random_state=42\n",
    "        )\n",
    "        lr_model.fit(X_train_scaled, train_labels)\n",
    "\n",
    "        # ê²€ì¦ ë°ì´í„°ë¡œ ì œëŒ€ë¡œ ëœ í‰ê°€\n",
    "        print(\"ê²€ì¦ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ...\")\n",
    "        X_val_combined, X_val_statistical, _ = create_a100_combined_features(\n",
    "            val_texts, model, tokenizer, device, batch_size=16\n",
    "        )\n",
    "        X_val_scaled = scaler.transform(X_val_statistical)\n",
    "\n",
    "        # ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "        print(\"ì•™ìƒë¸” ê²€ì¦ ì„±ëŠ¥ í‰ê°€...\")\n",
    "        xgb_pred = xgb_model.predict_proba(X_val_combined)[:, 1]\n",
    "        lr_pred = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "        # ê°„ë‹¨í•œ í‰ê·  ì•™ìƒë¸”\n",
    "        val_predictions = 0.7 * xgb_pred + 0.3 * lr_pred\n",
    "        val_auc = roc_auc_score(val_labels, val_predictions)\n",
    "\n",
    "        print(f\"ê²€ì¦ AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # ê³¼ì í•© ê°ì§€\n",
    "        if val_auc > 0.99:\n",
    "            print(\"ì—¬ì „íˆ ê³¼ì í•© ì˜ì‹¬! ëª¨ë¸ì„ ë” ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.\")\n",
    "            # ë‹¨ì¼ ëª¨ë¸ë§Œ ì‚¬ìš©\n",
    "            val_predictions = xgb_pred\n",
    "            val_auc = roc_auc_score(val_labels, val_predictions)\n",
    "            print(f\"ë‹¨ì¼ XGBoost AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # í›ˆë ¨ ë°ì´í„°ì˜ ì‹¤ì œ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "        train_ai_ratio = train['generated'].mean()\n",
    "        print(f\"ì „ì²´ í›ˆë ¨ ë°ì´í„° AI ë¹„ìœ¨: {train_ai_ratio:.3f}\")\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...\")\n",
    "        test_predictions = []\n",
    "        test_ids = []\n",
    "\n",
    "        # ê°„ë‹¨í•œ ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(test), batch_size):\n",
    "            batch_test = test.iloc[i:i+batch_size]\n",
    "            batch_texts = batch_test['paragraph_text'].tolist()\n",
    "            batch_ids = batch_test['ID'].tolist()\n",
    "\n",
    "            # íŠ¹ì„± ì¶”ì¶œ\n",
    "            try:\n",
    "                X_test_combined, X_test_statistical, _ = create_a100_combined_features(\n",
    "                    batch_texts, model, tokenizer, device, batch_size=8\n",
    "                )\n",
    "                X_test_scaled = scaler.transform(X_test_statistical)\n",
    "\n",
    "                # ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "                test_xgb_pred = xgb_model.predict_proba(X_test_combined)[:, 1]\n",
    "                test_lr_pred = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "                batch_pred = 0.7 * test_xgb_pred + 0.3 * test_lr_pred\n",
    "\n",
    "                test_predictions.extend(batch_pred)\n",
    "                test_ids.extend(batch_ids)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ë°°ì¹˜ {i} ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}\")\n",
    "                # ê¸°ë³¸ê°’ìœ¼ë¡œ í›ˆë ¨ ë°ì´í„° AI ë¹„ìœ¨ ì‚¬ìš©\n",
    "                batch_pred = [train_ai_ratio] * len(batch_texts)\n",
    "                test_predictions.extend(batch_pred)\n",
    "                test_ids.extend(batch_ids)\n",
    "\n",
    "        # í´ë˜ìŠ¤ ë¶„í¬ ë§ì¶¤ í›„ì²˜ë¦¬\n",
    "        print(\"í´ë˜ìŠ¤ ë¶„í¬ ë§ì¶¤ í›„ì²˜ë¦¬...\")\n",
    "        test_predictions = np.array(test_predictions)\n",
    "\n",
    "        # ì˜ˆì¸¡ê°’ì„ í›ˆë ¨ ë°ì´í„° ë¶„í¬ì— ë§ê²Œ ì¡°ì •\n",
    "        current_mean = test_predictions.mean()\n",
    "        target_mean = train_ai_ratio\n",
    "\n",
    "        print(f\"   í˜„ì¬ ì˜ˆì¸¡ í‰ê· : {current_mean:.3f}\")\n",
    "        print(f\"   ëª©í‘œ í‰ê· : {target_mean:.3f}\")\n",
    "\n",
    "        # ì„ í˜• ë³€í™˜ìœ¼ë¡œ ë¶„í¬ ì¡°ì •\n",
    "        if current_mean > 0:\n",
    "            scale_factor = target_mean / current_mean\n",
    "            test_predictions_adjusted = test_predictions * scale_factor\n",
    "            test_predictions_adjusted = np.clip(test_predictions_adjusted, 0, 1)\n",
    "        else:\n",
    "            test_predictions_adjusted = np.full_like(test_predictions, target_mean)\n",
    "\n",
    "        print(f\"   ì¡°ì • í›„ í‰ê· : {test_predictions_adjusted.mean():.3f}\")\n",
    "\n",
    "        # ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': test_ids,\n",
    "            'generated': test_predictions_adjusted\n",
    "        })\n",
    "\n",
    "        # ID ìˆœì„œë¡œ ì •ë ¬\n",
    "        submission = submission.sort_values('ID').reset_index(drop=True)\n",
    "        submission.to_csv('a100_fixed_submission.csv', index=False)\n",
    "\n",
    "        print(\"ê³¼ì í•© ë°©ì§€ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "        print(\"ê°œì„ ëœ ì„±ëŠ¥ ìš”ì•½:\")\n",
    "        print(f\"   ê²€ì¦ AUC: {val_auc:.4f} (ê³¼ì í•© ë°©ì§€ë¨)\")\n",
    "        print(f\"   ì˜ˆì¸¡ í†µê³„:\")\n",
    "        print(f\"      - í‰ê· : {submission['generated'].mean():.4f}\")\n",
    "        print(f\"      - í‘œì¤€í¸ì°¨: {submission['generated'].std():.4f}\")\n",
    "        print(f\"      - ìµœì†Œê°’: {submission['generated'].min():.4f}\")\n",
    "        print(f\"      - ìµœëŒ€ê°’: {submission['generated'].max():.4f}\")\n",
    "        print(f\"ì œì¶œ íŒŒì¼: a100_fixed_submission.csv\")\n",
    "\n",
    "        print(f\"\\nê³¼ì í•© ë°©ì§€ ê°œì„ ì‚¬í•­:\")\n",
    "        print(f\"   ì œëŒ€ë¡œ ëœ train/validation split\")\n",
    "        print(f\"   ê°„ì†Œí™”ëœ ì•™ìƒë¸” (2ê°œ ëª¨ë¸)\")\n",
    "        print(f\"   ì •ê·œí™” ê°•í™”\")\n",
    "        print(f\"   í´ë˜ìŠ¤ ë¶„í¬ ë§ì¶¤\")\n",
    "        print(f\"   ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„ \")\n",
    "\n",
    "        if val_auc < 0.99 and val_auc > 0.6:\n",
    "            print(f\"\\nì •ìƒì ì¸ ê²€ì¦ ì„±ëŠ¥! ë¦¬ë”ë³´ë“œ ì„±ëŠ¥ ê°œì„  ê¸°ëŒ€ë¨!\")\n",
    "\n",
    "        return submission\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ê³¼ì í•© ë°©ì§€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        print(f\"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"==ê³¼ì í•© ë°©ì§€ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ==\")\n",
    "    print(\"ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
    "    print(\"   - Title ê¸°ì¤€ train/validation split (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\")\n",
    "    print(\"   - ê°„ì†Œí™”ëœ ì•™ìƒë¸” (XGBoost + LogisticRegression)\")\n",
    "    print(\"   - ì •ê·œí™” ê°•í™” (ê³¼ì í•© ë°©ì§€)\")\n",
    "    print(\"   - í´ë˜ìŠ¤ ë¶„í¬ ë§ì¶¤ í›„ì²˜ë¦¬\")\n",
    "    print(\"   - ë©”ëª¨ë¦¬ ìµœì í™”\")\n",
    "    print()\n",
    "    print(\"ì‹¤í–‰ ë°©ë²•:\")\n",
    "    print(\"   exec(open('run_a100_fixed.py').read())\")\n",
    "    print(\"   run_a100_fixed_training_and_inference()\")\n",
    "    print()\n",
    "    print(\"ì˜ˆìƒ ê°œì„ ì‚¬í•­:\")\n",
    "    print(\"   - ê²€ì¦ AUC: 0.6~0.8 (ê³¼ì í•© ì—†ìŒ)\")\n",
    "    print(\"   - ë¦¬ë”ë³´ë“œ ì„±ëŠ¥: 0.5+ (ëœë¤ ì´ìƒ)\")\n",
    "    print(\"   - ì¼ë°˜í™” ì„±ëŠ¥: í¬ê²Œ ê°œì„ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff19e86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_a100_fixed_training_and_inference()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
